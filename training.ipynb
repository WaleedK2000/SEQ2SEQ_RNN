{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./training_data/Roman-Urdu.txt', 'r', encoding='utf-8') as f:\n",
    "    romanUrduLines = f.readlines()[:200]\n",
    "\n",
    "with open('./training_data/Urdu.txt', 'r', encoding='utf-8') as f:\n",
    "    urduLines = f.readlines()[:200]\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Class to load data\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.source = open('./training_data/Roman-Urdu.txt', 'r', encoding='utf-8').read().split('\\n')\n",
    "        self.target = open('./training_data/Urdu.txt', 'r', encoding='utf-8').read().split('\\n')\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        source_sample = self.source[idx]\n",
    "        target_sample = self.target[idx]\n",
    "\n",
    "        return source_sample, target_sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yourDataset = Dataset()\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "        yourDataset,\n",
    "        batch_size=8,\n",
    "        num_workers=0,\n",
    "        shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataloader.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./training_data/Roman-Urdu.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    roman_urdu_data = f.read()\n",
    "\n",
    "with open(\"./training_data/Urdu.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    urdu_data = f.read()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['وکیل ', 'وکیل    ', 'وکیل     ایک ایسی شخصیت کو کہا جاتا ہے کہ جو دوسرے  اپنے صارف  کی جانب سے یا اسکی بابت گفتگو کرے   اس مضمون میں یہ گفتگو قانون سے متعلق تصور کی گئی ہے اور اس وجہ سے یہ مضمون صرف قانونی وکلا  کے بارے میں ذکر کرتا ہے', 'وکیل  قانون ', 'وکیل  قانون  ایک شخص جسے دوسرے شخص کی جگہ کام کرنے یا     کی نمائندگی کرنے کا اختیار حاصل ہوتا ہے  وکیل  سفر  ایک شخص جو تعطیلات اور سفر کا بندوبست کرتا ہے  خفیہ وکیل  ایک جاسوس ', 'وکیل بازار ضلع   لاطینی       ترکمانستان کا ایک ترکمانستان کے اضلاع جو صوبہ ماری میں واقع ہے', 'وکیل والا ریلوے اسٹیشن وکیل والا میں واقع ہے', 'وکیل والا ریلوے اسٹیشن پاکستان میں واقع ہے', 'وکیپیڈیا', 'وکیپیڈیا انگریزی پر یہ تصویر ميں نے ہی اپلوڈ کی تھی', 'وکیپیڈیا خود کسی بھی ترتیب کو نافذ نہیں کرتا', 'وکیپیڈیا لوگو کے لیے نستعلیق خطاطی', 'وکیپیڈیا میں اس سطر سے آپ تصاویر سادہ طریقے سے شامل کرسکتے ہیں ', 'وکیپیڈیا میں خوش آمدید', 'وکیپیڈیا میں کسی کے کو اس بات کی وجہ سے فوقیت حاصل نہیں کہ اس نے کس قدر وکیپیڈیا میں اضافہ کیا ہے', 'وکیپیڈیا پالیسی ', 'وکیپیڈیا پر صرف   ہزار مضمون ہیں جن میں سے اصل مضمون شاید   ہزار ہونگے', 'وکیپیڈیا پر ہر صارف کا ابنا ایک صفحہ ہوتا ہے', 'وکیپیڈیا کے اعدادوشمار', 'وکیپیڈیا کے صارفین کی اصل تصاویر', 'وکیپیڈیا کے کسی بھی صفحہ کے دائیں جانب  تلاش کا خانہ  نظر آتا ہے', 'وگر نہ دل تو اسے روکنا ہی چاہتا ہے', 'وگن شہر ضلع لاڑکانہ سندھ کا ایک خوبصورت شہر ہے  یہ انڈس ہائی وے پر لاڑکانہ سے   کلومیٹر کے فاصلہ پر واقع ہے', 'وھاں کے رہنے والوں کا تمدن بھی حضرت نوح علیہ السلام  حضرت ہود علیہ السلام اور حضرت شعیب علیہ السلام کی اقوام سے زیادہ ترقی یافتہ تھا', 'وہ       اور وہ ام  الکلامی', 'وہ     میں آئی سی سی کی جانب سے ٹی    فارمیٹ میں  سال کی بہترین کار کردگی  کا اعزاز بھی حاصل کر چکا ہے جو اسے ویسٹ انڈیز کے خلاف   گیندوں پر   دوڑیں  رنز  بنا نے پر ملا ', 'وہ    تا    الپ ارسلان کے دور حکومت اور    تا    ملک شاہ اول کے دور حکومت میں وزارت کے عہدے پر فائز رہا', 'وہ    تا    میں لاہور ہائی کورٹ بار ایسوسی ایشن کے سیکرٹری منتخب ہوئے', 'وہ    دسمبر    کو مختصر علالت کے بعد کراچی کے ایک امراض قلب کے ہسپتال میں خالق  حقیقی سے جاملے  اور وہیں نارتھ کراچی کے محمد شاہ قبرستان میں آسودہ خاک ہیں', 'وہ    سے    تک بھارت کے وزیر اعظم رہے', 'وہ    سے لے کر    تک حکومت پاکستان کے وکیل رہے', 'وہ    میں   برس کی عمر میں انتقال کرگئے', 'وہ    میں استنبول میں پیدا ہوئے', 'وہ    میں اقوام متحدہ گئے اور کشمیر پر اپنا موقف بیان کیا', 'وہ    میں امریکی فضائیہ کے شائع کردہ پوسٹر تاریخ کے   عظیم ترین ہوا باز میں واحد خاتون تھیں', 'وہ    میں انسان کی قوت  مدافعت کو کمزور کر دینے والی ایک مہلک بیماری   میں مبتلا ہو گئے تھے', 'وہ    میں اوول کی تاریخی کامیابی حاصل کرنے والی پاکستانی ٹیم کا بھی حصہ تھے', 'وہ    میں بھارتیہ جنتا پارٹی میں شامل ہوتے ہوئے ریاستی سیاست میں کود پڑے', 'وہ    میں دمشق میں پیدا ہوئے', 'وہ    میں مرکزی اردو بورڈ کے ڈائریکٹر مقرر ہوئے جو بعد میں اردو سائنس بورڈ میں تبدیل ہوگیا', 'وہ    میں مصر کی بستی محمودیہ کے ایک علم دوست اور دیندار گھرانے میں پیدا ہوئے', 'وہ    میں وزیر تعلیم بھی رہ چکے ہیں', 'وہ    میں کراچی میونسپلٹی کے رکن بنے اور    تک اس کے رکن رہے', 'وہ    میں کینیڈا میں ڈیڑھ سال گزارے کے بعد پاکستان واپس آئیں لیکن گوجرانوالہ میں ایک شو کے دوران انہیں پولیس نے حراست میں لے لیا اور ان پر عریانی اور لچر ڈانس کا الزام لگا کر حوالات میں بھیج دیا گیا', 'وہ    کو میرٹھ میں مولانا عبدالعلیم صدیقی کے گھر پیدا ہوئے', 'وہ    کے اوائل میں حمایت علی کراچی آکر ریڈیو پاکستان کا حصہ بنے', 'وہ    کے عام انتخابا   میں ایک بار پھر منتخب ہو ئے اور   اپریل    سے ڈپٹی اسپیکر بلوچستان اسمبلی کے عہدے پر فائز ہیں', 'وہ    کے عام انتخابات میں پشین کے حلقہ    سے رکن بلوچستان صوبائی اسمبلی منتخب ہو ئے ', 'وہ   اسلامی جمہوریہ پاکستان  کے سخت مخالف تھے', 'وہ   اپریل    سے  اکتوبر    تک بلوچستان کے وزیر اعلی  بھی رہے']\n"
     ]
    }
   ],
   "source": [
    "print(urdu_data.splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "roman_urdu_lines = roman_urdu_data.splitlines()\n",
    "urdu_lines = urdu_data.splitlines()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output of urdu text has empty spaces which we need to remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['وکیل', '', '', '', '', 'ایک', 'ایسی', 'شخصیت', 'کو', 'کہا']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urdu_lines[2].split(' ')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "وکیل   ایک ایسی شخصیت کو کہا جاتا ہے کہ جو دوسرے اپنے صارف کی جانب سے یا اسکی بابت گفتگو کرے  اس مضمون میں یہ گفتگو قانون سے متعلق تصور کی گئی ہے اور اس وجہ سے یہ مضمون صرف قانونی وکلا کے بارے میں ذکر کرتا ہے\n"
     ]
    }
   ],
   "source": [
    "print(urdu_lines[2].replace('  ',' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['وکیل', 'ایک', 'ایسی', 'شخصیت', 'کو', 'کہا', 'جاتا', 'ہے', 'کہ', 'جو']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word for word in urdu_lines[2].split(' ') if word != ''][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, line in enumerate(urdu_lines):\n",
    "    # Remove empty spaces for each line and also truncate to 10 words\n",
    "    urdu_lines[i] = ' '.join([word for word in line.split(' ') if word != ''][:10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that now empty spaces are gone and the sentences are truncated to 10 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['وکیل', 'ایک', 'ایسی', 'شخصیت', 'کو', 'کہا', 'جاتا', 'ہے', 'کہ', 'جو']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urdu_lines[2].split(' ')\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do the same for roman urdu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, line in enumerate(roman_urdu_lines):\n",
    "    # Remove empty spaces for each line and also truncate to 10 words\n",
    "    roman_urdu_lines[i] = ' '.join([word for word in line.split(' ') if word != ''][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wakeel',\n",
       " 'aik',\n",
       " 'aisi',\n",
       " 'shakhsiyat',\n",
       " 'ko',\n",
       " 'kaha',\n",
       " 'jata',\n",
       " 'hai',\n",
       " 'ke',\n",
       " 'jo']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roman_urdu_lines[2].split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wakeel', 'wakeel', 'wakeel aik aisi shakhsiyat ko kaha jata hai ke jo dosray apne Sarif ki janib se ya uski baabat guftagu kere is mazmoon mein yeh guftagu qanoon se mutaliq tasawwur ki gayi hai aur is wajah se yeh mazmoon sirf qanooni wukla ke baray mein zikar karta hai', 'wakeel qanoon', 'wakeel qanoon 1 shakhs jisay dosray shakhs ki jagah kaam karne ya ki numaindagi karne ka ikhtiyar haasil hota hai wakeel safar 1 shakhs jo tatilat aur safar ka bandobast karta hai khufia wakeel aik jasoos', 'wakeel bazaar zila lateeni turkmenistan ka aik turkmenistan ke azlaa jo soobah maari mein waqay hai', 'wakeel wala railway station wakeel wala mein waqay hai', 'wakeel wala railway station Pakistan mein waqay hai', 'wikipedia', 'wikipedia angrezi par yeh tasweer mein ne hi aplod ki thi', 'wikipedia khud kisi bhi tarteeb ko nafiz nahi karta', 'wikipedia logo ke liye nastalik khtati', 'wikipedia mein is satar se aap tasaveer saada tareeqay se shaamil kar saktay hain', 'wikipedia mein khush aamdeed', 'wikipedia mein kisi ke ko is baat ki wajah se foqiat haasil nahi ke is ne kis qader wikipedia mein izafah kya hai', 'wikipedia policy', 'wikipedia par sirf hazaar mazmoon hain jin mein se asal mazmoon shayad hazaar hunge', 'wikipedia par har Sarif ka abna aik safha hota hai', 'wikipedia ke adad o shumaar', 'wikipedia ke sarfeen ki asal tasaveer', 'wikipedia ke kisi bhi safha ke dayen janib talaash ka khanah nazar aata hai', 'wagar nah dil to usay rokna hi chahta hai', 'vgn shehar zila larkana Sindh ka aik khobsorat shehar hai yeh indus high way par larkana se kilometer ke faasla par waqay hai', 'vhan ke rehne walon ka tamaddun bhi hazrat Nooh aleh salam hazrat Hood aleh salam aur hazrat Shoiab aleh salam ki aqwam se ziyada taraqqi Yafta tha', 'woh aur woh umm alklami', 'woh mein aayi si si ki janib se tea farmit mein saal ki behtareen car kardagi ka aizaz bhi haasil kar chuka hai jo usay west indies ke khilaaf gaindon par dorhin runs bana ne par mila', 'woh taa Alap Arsalan ke daur hukoomat aur taa malik Shah awwal ke daur hukoomat mein wizarat ke ohday par Faiz raha', 'woh taa mein Lahore high court baar association ke secretary muntakhib hue', 'woh decemeber ko mukhtasir alalat ke baad Karachi ke aik amraaz qalb ke hospital mein khaaliq haqeeqi se jamilay aur wahein north Karachi ke Mohammad Shah qabrustan mein aasooda khaak hain', 'woh se taq Bharat ke Wazeer e Azam rahay', 'woh se le kar taq hukoomat Pakistan ke wakeel rahay', 'woh mein baras ki Umar mein intqaal kargaye', 'woh mein istanbul mein peda hue', 'woh mein aqwam mutahidda gay aur Kashmir par apna muaqqaf bayan kya', 'woh mein Amrici fazaiya ke shaya kardah poster tareekh ke azeem tareen sun-hwa baz mein wahid khatoon theen', 'woh mein ensaan ki qowat mudafat ko kamzor kar dainay wali aik mohlik bemari mein mubtala ho gay they', 'woh mein Oval ki tareekhi kamyabi haasil karne wali Pakistani team ka bhi hissa they', 'woh mein bhartih jnta party mein shaamil hotay hue reyasti siyasat mein kood parre', 'woh mein Dimashq mein peda hue', 'woh mein markazi urdu board ke director muqarrar hue jo baad mein urdu science board mein tabdeel hogaya', 'woh mein misar ki bastii mhmodih ke aik ilm dost aur deendar gharane mein peda hue', 'woh mein wazeer taleem bhi reh chuke hain', 'woh mein Karachi myonsplti ke rukan banay aur taq is ke rukan rahay', 'woh mein canada mein daidh saal guzaray ke baad Pakistan wapas ayen lekin Gujranwala mein aik show ke douran inhen police ne hirasat mein le liya aur un par uryani aur lachar dance ka ilzaam laga kar hawalaat mein bhaij diya gaya', 'woh ko mirth mein molana abdalalim Siddiqui ke ghar peda hue', 'woh ke awail mein himayat Ali Karachi aakar radio Pakistan ka hissa banay', 'woh ke aam antkhaba mein aik baar phir muntakhib ho ye aur April se deputy speaker Balochistan assembly ke ohday par Faiz hain', 'woh ke aam intikhabaat mein Pasheen ke halqa se rukan Balochistan sobai assembly muntakhib ho ye', 'woh islami jamhooria Pakistan ke sakht mukhalif they', 'woh April se october taq Balochistan ke wazeer Alla bhi rahay'] ['وکیل', 'قانون']\n"
     ]
    }
   ],
   "source": [
    "print(roman_urdu_lines, urdu_lines[3].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUniqueWords(lines):\n",
    "\n",
    "    vocab = []\n",
    "    for i in lines:\n",
    "        toks = i.split()\n",
    "\n",
    "        for j in toks:\n",
    "            if j not in vocab:\n",
    "                vocab.append(j)\n",
    "\n",
    "    return vocab"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Vocab of roman urdu and urdu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext.vocab as vocab\n",
    "\n",
    "vocab_roman = getUniqueWords(roman_urdu_lines)\n",
    "vocab_urdu = getUniqueWords(urdu_lines)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Urdu Vocab only first 35 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['وکیل',\n",
       " 'ایک',\n",
       " 'ایسی',\n",
       " 'شخصیت',\n",
       " 'کو',\n",
       " 'کہا',\n",
       " 'جاتا',\n",
       " 'ہے',\n",
       " 'کہ',\n",
       " 'جو',\n",
       " 'قانون',\n",
       " 'شخص',\n",
       " 'جسے',\n",
       " 'دوسرے',\n",
       " 'کی',\n",
       " 'جگہ',\n",
       " 'کام',\n",
       " 'بازار',\n",
       " 'ضلع',\n",
       " 'لاطینی',\n",
       " 'ترکمانستان',\n",
       " 'کا',\n",
       " 'کے',\n",
       " 'اضلاع',\n",
       " 'والا',\n",
       " 'ریلوے',\n",
       " 'اسٹیشن',\n",
       " 'میں',\n",
       " 'واقع',\n",
       " 'پاکستان',\n",
       " 'وکیپیڈیا',\n",
       " 'انگریزی',\n",
       " 'پر',\n",
       " 'یہ',\n",
       " 'تصویر']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(vocab_urdu))\n",
    "vocab_urdu[:35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['wakeel',\n",
       " 'aik',\n",
       " 'aisi',\n",
       " 'shakhsiyat',\n",
       " 'ko',\n",
       " 'kaha',\n",
       " 'jata',\n",
       " 'hai',\n",
       " 'ke',\n",
       " 'jo',\n",
       " 'qanoon',\n",
       " '1',\n",
       " 'shakhs',\n",
       " 'jisay',\n",
       " 'dosray',\n",
       " 'ki',\n",
       " 'jagah',\n",
       " 'kaam',\n",
       " 'bazaar',\n",
       " 'zila',\n",
       " 'lateeni',\n",
       " 'turkmenistan',\n",
       " 'ka',\n",
       " 'azlaa',\n",
       " 'wala',\n",
       " 'railway',\n",
       " 'station',\n",
       " 'mein',\n",
       " 'waqay',\n",
       " 'Pakistan',\n",
       " 'wikipedia',\n",
       " 'angrezi',\n",
       " 'par',\n",
       " 'yeh',\n",
       " 'tasweer']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(vocab_roman))\n",
    "vocab_roman[:35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_zip = list(zip(vocab_roman, vocab_urdu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('wakeel', 'وکیل'), ('aik', 'ایک'), ('aisi', 'ایسی'), ('shakhsiyat', 'شخصیت'), ('ko', 'کو'), ('kaha', 'کہا'), ('jata', 'جاتا'), ('hai', 'ہے'), ('ke', 'کہ'), ('jo', 'جو'), ('qanoon', 'قانون'), ('1', 'شخص'), ('shakhs', 'جسے'), ('jisay', 'دوسرے'), ('dosray', 'کی'), ('ki', 'جگہ'), ('jagah', 'کام'), ('kaam', 'بازار'), ('bazaar', 'ضلع'), ('zila', 'لاطینی'), ('lateeni', 'ترکمانستان'), ('turkmenistan', 'کا'), ('ka', 'کے'), ('azlaa', 'اضلاع'), ('wala', 'والا'), ('railway', 'ریلوے'), ('station', 'اسٹیشن'), ('mein', 'میں'), ('waqay', 'واقع'), ('Pakistan', 'پاکستان'), ('wikipedia', 'وکیپیڈیا'), ('angrezi', 'انگریزی'), ('par', 'پر'), ('yeh', 'یہ'), ('tasweer', 'تصویر'), ('ne', 'ميں'), ('hi', 'نے'), ('aplod', 'ہی'), ('khud', 'اپلوڈ'), ('kisi', 'خود'), ('bhi', 'کسی'), ('tarteeb', 'بھی'), ('nafiz', 'ترتیب'), ('nahi', 'نافذ'), ('karta', 'نہیں'), ('logo', 'کرتا'), ('liye', 'لوگو'), ('nastalik', 'لیے'), ('khtati', 'نستعلیق'), ('is', 'خطاطی'), ('satar', 'اس'), ('se', 'سطر'), ('aap', 'سے'), ('tasaveer', 'آپ'), ('saada', 'تصاویر'), ('tareeqay', 'سادہ'), ('khush', 'طریقے'), ('aamdeed', 'خوش'), ('baat', 'آمدید'), ('wajah', 'بات'), ('policy', 'وجہ'), ('sirf', 'پالیسی'), ('hazaar', 'صرف'), ('mazmoon', 'ہزار'), ('hain', 'مضمون'), ('jin', 'ہیں'), ('asal', 'جن'), ('har', 'اصل'), ('Sarif', 'ہر'), ('abna', 'صارف'), ('safha', 'ابنا'), ('hota', 'صفحہ'), ('adad', 'ہوتا'), ('o', 'اعدادوشمار'), ('shumaar', 'صارفین'), ('sarfeen', 'دائیں'), ('dayen', 'جانب'), ('janib', 'تلاش'), ('talaash', 'وگر'), ('wagar', 'نہ'), ('nah', 'دل'), ('dil', 'تو'), ('to', 'اسے'), ('usay', 'روکنا'), ('rokna', 'چاہتا'), ('chahta', 'وگن'), ('vgn', 'شہر'), ('shehar', 'لاڑکانہ'), ('larkana', 'سندھ'), ('Sindh', 'خوبصورت'), ('khobsorat', 'وھاں'), ('vhan', 'رہنے'), ('rehne', 'والوں'), ('walon', 'تمدن'), ('tamaddun', 'حضرت'), ('hazrat', 'نوح'), ('Nooh', 'علیہ'), ('aleh', 'وہ'), ('woh', 'اور'), ('aur', 'ام'), ('umm', 'الکلامی'), ('alklami', 'آئی'), ('aayi', 'سی'), ('si', 'ٹی'), ('tea', 'فارمیٹ'), ('farmit', 'تا'), ('taa', 'الپ'), ('Alap', 'ارسلان'), ('Arsalan', 'دور'), ('daur', 'حکومت'), ('hukoomat', 'ملک'), ('malik', 'لاہور'), ('Lahore', 'ہائی'), ('high', 'کورٹ'), ('court', 'بار'), ('baar', 'ایسوسی'), ('association', 'ایشن'), ('secretary', 'دسمبر'), ('decemeber', 'مختصر'), ('mukhtasir', 'علالت'), ('alalat', 'بعد'), ('baad', 'کراچی'), ('Karachi', 'تک'), ('taq', 'بھارت'), ('Bharat', 'وزیر'), ('Wazeer', 'اعظم'), ('e', 'رہے'), ('Azam', 'لے'), ('rahay', 'کر'), ('le', 'برس'), ('kar', 'عمر'), ('baras', 'انتقال'), ('Umar', 'کرگئے'), ('intqaal', 'استنبول'), ('kargaye', 'پیدا'), ('istanbul', 'ہوئے'), ('peda', 'اقوام'), ('hue', 'متحدہ'), ('aqwam', 'گئے'), ('mutahidda', 'کشمیر'), ('gay', 'اپنا'), ('Kashmir', 'موقف'), ('apna', 'امریکی'), ('muaqqaf', 'فضائیہ'), ('Amrici', 'شائع'), ('fazaiya', 'کردہ'), ('shaya', 'پوسٹر'), ('kardah', 'تاریخ'), ('poster', 'انسان'), ('tareekh', 'قوت'), ('ensaan', 'مدافعت'), ('qowat', 'کمزور'), ('mudafat', 'دینے'), ('kamzor', 'اوول'), ('dainay', 'تاریخی'), ('Oval', 'کامیابی'), ('tareekhi', 'حاصل'), ('kamyabi', 'کرنے'), ('haasil', 'والی'), ('karne', 'پاکستانی'), ('wali', 'بھارتیہ'), ('Pakistani', 'جنتا'), ('bhartih', 'پارٹی'), ('jnta', 'شامل'), ('party', 'ہوتے'), ('shaamil', 'ریاستی'), ('hotay', 'دمشق'), ('reyasti', 'مرکزی'), ('Dimashq', 'اردو'), ('markazi', 'بورڈ'), ('urdu', 'ڈائریکٹر'), ('board', 'مقرر'), ('director', 'مصر'), ('muqarrar', 'بستی'), ('misar', 'محمودیہ'), ('bastii', 'علم'), ('mhmodih', 'دوست'), ('ilm', 'تعلیم'), ('dost', 'رہ'), ('wazeer', 'چکے'), ('taleem', 'میونسپلٹی'), ('reh', 'رکن'), ('chuke', 'بنے'), ('myonsplti', 'کینیڈا'), ('rukan', 'ڈیڑھ'), ('banay', 'سال'), ('canada', 'گزارے'), ('daidh', 'میرٹھ'), ('saal', 'مولانا'), ('guzaray', 'عبدالعلیم'), ('mirth', 'صدیقی'), ('molana', 'گھر'), ('abdalalim', 'اوائل'), ('Siddiqui', 'حمایت'), ('ghar', 'علی'), ('awail', 'آکر'), ('himayat', 'ریڈیو'), ('Ali', 'عام'), ('aakar', 'انتخابا'), ('radio', 'پھر'), ('aam', 'منتخب'), ('antkhaba', 'ہو'), ('phir', 'انتخابات'), ('muntakhib', 'پشین'), ('ho', 'حلقہ'), ('intikhabaat', 'اسلامی'), ('Pasheen', 'جمہوریہ'), ('halqa', 'سخت'), ('islami', 'مخالف'), ('jamhooria', 'تھے'), ('sakht', 'اپریل'), ('mukhalif', 'اکتوبر'), ('they', 'بلوچستان'), ('April', 'اعلی')]\n"
     ]
    }
   ],
   "source": [
    "print(f_zip)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary that maps each word to an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'wakeel': 0, 'aik': 1, 'aisi': 2, 'shakhsiyat': 3, 'ko': 4, 'kaha': 5, 'jata': 6, 'hai': 7, 'ke': 8, 'jo': 9, 'qanoon': 10, '1': 11, 'shakhs': 12, 'jisay': 13, 'dosray': 14, 'ki': 15, 'jagah': 16, 'kaam': 17, 'bazaar': 18, 'zila': 19, 'lateeni': 20, 'turkmenistan': 21, 'ka': 22, 'azlaa': 23, 'wala': 24, 'railway': 25, 'station': 26, 'mein': 27, 'waqay': 28, 'Pakistan': 29, 'wikipedia': 30, 'angrezi': 31, 'par': 32, 'yeh': 33, 'tasweer': 34, 'ne': 35, 'hi': 36, 'aplod': 37, 'khud': 38, 'kisi': 39, 'bhi': 40, 'tarteeb': 41, 'nafiz': 42, 'nahi': 43, 'karta': 44, 'logo': 45, 'liye': 46, 'nastalik': 47, 'khtati': 48, 'is': 49, 'satar': 50, 'se': 51, 'aap': 52, 'tasaveer': 53, 'saada': 54, 'tareeqay': 55, 'khush': 56, 'aamdeed': 57, 'baat': 58, 'wajah': 59, 'policy': 60, 'sirf': 61, 'hazaar': 62, 'mazmoon': 63, 'hain': 64, 'jin': 65, 'asal': 66, 'har': 67, 'Sarif': 68, 'abna': 69, 'safha': 70, 'hota': 71, 'adad': 72, 'o': 73, 'shumaar': 74, 'sarfeen': 75, 'dayen': 76, 'janib': 77, 'talaash': 78, 'wagar': 79, 'nah': 80, 'dil': 81, 'to': 82, 'usay': 83, 'rokna': 84, 'chahta': 85, 'vgn': 86, 'shehar': 87, 'larkana': 88, 'Sindh': 89, 'khobsorat': 90, 'vhan': 91, 'rehne': 92, 'walon': 93, 'tamaddun': 94, 'hazrat': 95, 'Nooh': 96, 'aleh': 97, 'woh': 98, 'aur': 99, 'umm': 100, 'alklami': 101, 'aayi': 102, 'si': 103, 'tea': 104, 'farmit': 105, 'taa': 106, 'Alap': 107, 'Arsalan': 108, 'daur': 109, 'hukoomat': 110, 'malik': 111, 'Lahore': 112, 'high': 113, 'court': 114, 'baar': 115, 'association': 116, 'secretary': 117, 'decemeber': 118, 'mukhtasir': 119, 'alalat': 120, 'baad': 121, 'Karachi': 122, 'taq': 123, 'Bharat': 124, 'Wazeer': 125, 'e': 126, 'Azam': 127, 'rahay': 128, 'le': 129, 'kar': 130, 'baras': 131, 'Umar': 132, 'intqaal': 133, 'kargaye': 134, 'istanbul': 135, 'peda': 136, 'hue': 137, 'aqwam': 138, 'mutahidda': 139, 'gay': 140, 'Kashmir': 141, 'apna': 142, 'muaqqaf': 143, 'Amrici': 144, 'fazaiya': 145, 'shaya': 146, 'kardah': 147, 'poster': 148, 'tareekh': 149, 'ensaan': 150, 'qowat': 151, 'mudafat': 152, 'kamzor': 153, 'dainay': 154, 'Oval': 155, 'tareekhi': 156, 'kamyabi': 157, 'haasil': 158, 'karne': 159, 'wali': 160, 'Pakistani': 161, 'bhartih': 162, 'jnta': 163, 'party': 164, 'shaamil': 165, 'hotay': 166, 'reyasti': 167, 'Dimashq': 168, 'markazi': 169, 'urdu': 170, 'board': 171, 'director': 172, 'muqarrar': 173, 'misar': 174, 'bastii': 175, 'mhmodih': 176, 'ilm': 177, 'dost': 178, 'wazeer': 179, 'taleem': 180, 'reh': 181, 'chuke': 182, 'myonsplti': 183, 'rukan': 184, 'banay': 185, 'canada': 186, 'daidh': 187, 'saal': 188, 'guzaray': 189, 'mirth': 190, 'molana': 191, 'abdalalim': 192, 'Siddiqui': 193, 'ghar': 194, 'awail': 195, 'himayat': 196, 'Ali': 197, 'aakar': 198, 'radio': 199, 'aam': 200, 'antkhaba': 201, 'phir': 202, 'muntakhib': 203, 'ho': 204, 'intikhabaat': 205, 'Pasheen': 206, 'halqa': 207, 'islami': 208, 'jamhooria': 209, 'sakht': 210, 'mukhalif': 211, 'they': 212, 'April': 213, 'october': 214, 'Balochistan': 215, 'Alla': 216}\n",
      "{'وکیل': 0, 'ایک': 1, 'ایسی': 2, 'شخصیت': 3, 'کو': 4, 'کہا': 5, 'جاتا': 6, 'ہے': 7, 'کہ': 8, 'جو': 9, 'قانون': 10, 'شخص': 11, 'جسے': 12, 'دوسرے': 13, 'کی': 14, 'جگہ': 15, 'کام': 16, 'بازار': 17, 'ضلع': 18, 'لاطینی': 19, 'ترکمانستان': 20, 'کا': 21, 'کے': 22, 'اضلاع': 23, 'والا': 24, 'ریلوے': 25, 'اسٹیشن': 26, 'میں': 27, 'واقع': 28, 'پاکستان': 29, 'وکیپیڈیا': 30, 'انگریزی': 31, 'پر': 32, 'یہ': 33, 'تصویر': 34, 'ميں': 35, 'نے': 36, 'ہی': 37, 'اپلوڈ': 38, 'خود': 39, 'کسی': 40, 'بھی': 41, 'ترتیب': 42, 'نافذ': 43, 'نہیں': 44, 'کرتا': 45, 'لوگو': 46, 'لیے': 47, 'نستعلیق': 48, 'خطاطی': 49, 'اس': 50, 'سطر': 51, 'سے': 52, 'آپ': 53, 'تصاویر': 54, 'سادہ': 55, 'طریقے': 56, 'خوش': 57, 'آمدید': 58, 'بات': 59, 'وجہ': 60, 'پالیسی': 61, 'صرف': 62, 'ہزار': 63, 'مضمون': 64, 'ہیں': 65, 'جن': 66, 'اصل': 67, 'ہر': 68, 'صارف': 69, 'ابنا': 70, 'صفحہ': 71, 'ہوتا': 72, 'اعدادوشمار': 73, 'صارفین': 74, 'دائیں': 75, 'جانب': 76, 'تلاش': 77, 'وگر': 78, 'نہ': 79, 'دل': 80, 'تو': 81, 'اسے': 82, 'روکنا': 83, 'چاہتا': 84, 'وگن': 85, 'شہر': 86, 'لاڑکانہ': 87, 'سندھ': 88, 'خوبصورت': 89, 'وھاں': 90, 'رہنے': 91, 'والوں': 92, 'تمدن': 93, 'حضرت': 94, 'نوح': 95, 'علیہ': 96, 'وہ': 97, 'اور': 98, 'ام': 99, 'الکلامی': 100, 'آئی': 101, 'سی': 102, 'ٹی': 103, 'فارمیٹ': 104, 'تا': 105, 'الپ': 106, 'ارسلان': 107, 'دور': 108, 'حکومت': 109, 'ملک': 110, 'لاہور': 111, 'ہائی': 112, 'کورٹ': 113, 'بار': 114, 'ایسوسی': 115, 'ایشن': 116, 'دسمبر': 117, 'مختصر': 118, 'علالت': 119, 'بعد': 120, 'کراچی': 121, 'تک': 122, 'بھارت': 123, 'وزیر': 124, 'اعظم': 125, 'رہے': 126, 'لے': 127, 'کر': 128, 'برس': 129, 'عمر': 130, 'انتقال': 131, 'کرگئے': 132, 'استنبول': 133, 'پیدا': 134, 'ہوئے': 135, 'اقوام': 136, 'متحدہ': 137, 'گئے': 138, 'کشمیر': 139, 'اپنا': 140, 'موقف': 141, 'امریکی': 142, 'فضائیہ': 143, 'شائع': 144, 'کردہ': 145, 'پوسٹر': 146, 'تاریخ': 147, 'انسان': 148, 'قوت': 149, 'مدافعت': 150, 'کمزور': 151, 'دینے': 152, 'اوول': 153, 'تاریخی': 154, 'کامیابی': 155, 'حاصل': 156, 'کرنے': 157, 'والی': 158, 'پاکستانی': 159, 'بھارتیہ': 160, 'جنتا': 161, 'پارٹی': 162, 'شامل': 163, 'ہوتے': 164, 'ریاستی': 165, 'دمشق': 166, 'مرکزی': 167, 'اردو': 168, 'بورڈ': 169, 'ڈائریکٹر': 170, 'مقرر': 171, 'مصر': 172, 'بستی': 173, 'محمودیہ': 174, 'علم': 175, 'دوست': 176, 'تعلیم': 177, 'رہ': 178, 'چکے': 179, 'میونسپلٹی': 180, 'رکن': 181, 'بنے': 182, 'کینیڈا': 183, 'ڈیڑھ': 184, 'سال': 185, 'گزارے': 186, 'میرٹھ': 187, 'مولانا': 188, 'عبدالعلیم': 189, 'صدیقی': 190, 'گھر': 191, 'اوائل': 192, 'حمایت': 193, 'علی': 194, 'آکر': 195, 'ریڈیو': 196, 'عام': 197, 'انتخابا': 198, 'پھر': 199, 'منتخب': 200, 'ہو': 201, 'انتخابات': 202, 'پشین': 203, 'حلقہ': 204, 'اسلامی': 205, 'جمہوریہ': 206, 'سخت': 207, 'مخالف': 208, 'تھے': 209, 'اپریل': 210, 'اکتوبر': 211, 'بلوچستان': 212, 'اعلی': 213}\n"
     ]
    }
   ],
   "source": [
    "roman_word_to_ix = {word: i for i, word in enumerate(vocab_roman)}\n",
    "urdu_word_to_ix = {word: i for i, word in enumerate(vocab_urdu)}\n",
    "\n",
    "REVERSE_roman_word_to_ix =  {v: k for k, v in roman_word_to_ix.items()}\n",
    "REVERSE_urdu_word_to_ix =  {v: k for k, v in urdu_word_to_ix.items()}\n",
    "\n",
    "\n",
    "\n",
    "print(roman_word_to_ix)\n",
    "print(urdu_word_to_ix)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert To indeces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "urdu_word_indices = torch.tensor([urdu_word_to_ix[word] for word in vocab_urdu], dtype=torch.long)\n",
    "roman_word_indices = torch.tensor([roman_word_to_ix[word] for word in vocab_roman], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213])\n"
     ]
    }
   ],
   "source": [
    "print(urdu_word_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "        336, 337, 338])\n"
     ]
    }
   ],
   "source": [
    "print(roman_word_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "embedding = nn.Embedding(num_embeddings=500, embedding_dim=128)\n",
    "\n",
    "\n",
    "roman_embedded_words = embedding(roman_word_indices)\n",
    "urdu_embedded_words = embedding(urdu_word_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.9712,  0.6001,  1.3942,  ..., -0.9436, -0.8932, -0.1931],\n",
      "        [ 0.8179,  0.1684, -0.9584,  ..., -0.2299,  0.6909,  0.2241],\n",
      "        [ 0.4038,  0.2324,  1.0719,  ...,  0.9289, -1.4149,  0.6941],\n",
      "        ...,\n",
      "        [ 0.4305,  0.4337,  0.1545,  ...,  0.3729, -0.5917, -0.0610],\n",
      "        [-0.0384, -1.3454,  1.3223,  ...,  0.6655,  0.5884,  0.8072],\n",
      "        [ 0.4210, -2.2935, -0.1947,  ..., -0.2525, -2.0656,  1.9400]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(urdu_embedded_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.9712,  0.6001,  1.3942,  ..., -0.9436, -0.8932, -0.1931],\n",
      "        [ 0.8179,  0.1684, -0.9584,  ..., -0.2299,  0.6909,  0.2241],\n",
      "        [ 0.4038,  0.2324,  1.0719,  ...,  0.9289, -1.4149,  0.6941],\n",
      "        ...,\n",
      "        [-0.3086,  0.5473,  0.6639,  ...,  0.5194, -0.3119, -0.7241],\n",
      "        [-0.0522,  0.1437, -0.4276,  ..., -0.0088,  1.1825, -0.4129],\n",
      "        [-2.1778, -0.1771, -0.0880,  ..., -0.9535, -4.3381, -0.9850]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(roman_embedded_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we write the encorder Rnn class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size,hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "    # loop 10 times to implement 10 cells of rnn\n",
    "        for i in range(10):\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1):\n",
    "        super(Attention, self).__init__()\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.softmax = torch.nn.Softmax(dim=-1)\n",
    "        self.scale = torch.sqrt(torch.tensor(d_model, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        # q, k, v have shape (batch_size, seq_len, d_model)\n",
    "        scores = torch.matmul(q, k.transpose(-1, -2)) / self.scale\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        attention_weights = self.softmax(scores)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        # Compute weighted sum of values\n",
    "        output = torch.matmul(attention_weights, v)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is our Decoder Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, output_size, hidden_size, num_layers = 10):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size*2, hidden_size)\n",
    "        self.attention = Attention(hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden, encorder_output):\n",
    "       \n",
    "        output = self.embedding(input).view(1, 1, self.hidden_size)\n",
    "        context, attention_weights = self.attention(hidden[-1].unsqueeze(0), encorder_output, encorder_output)\n",
    "        \n",
    "        output = torch.cat((output[:, :, :self.hidden_size], context), dim=2)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        # output = torch.cat((output, context), dim=2)\n",
    "\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DecoderRNN(nn.Module):\n",
    "#     def __init__(self, output_size, hidden_size, num_layers = 10):\n",
    "#         super(DecoderRNN, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "\n",
    "#         self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "#         self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "#         self.attention = Attention\n",
    "#         self.out = nn.Linear(hidden_size, output_size)\n",
    "#         self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "#     def forward(self, input, hidden, encorder_output):\n",
    "#         output = self.embedding(input).view(1, 1, -1)\n",
    "#         output = F.relu(output)\n",
    "#         output, hidden = self.gru(output, hidden)\n",
    "#         output = self.softmax(self.out(output[0]))\n",
    "#         return output, hidden\n",
    "    \n",
    "#     def initHidden(self):\n",
    "#         return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wakeel aik aisi shakhsiyat ko kaha jata hai ke jo'"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roman_urdu_lines[2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make pairs for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = list(zip(roman_urdu_lines, urdu_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('wikipedia khud kisi bhi tarteeb ko nafiz nahi karta',\n",
       " 'وکیپیڈیا خود کسی بھی ترتیب کو نافذ نہیں کرتا')"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_from_sentence(idx, sentence):\n",
    "    return [idx[word] for word in sentence.split(' ')]\n",
    "\n",
    "def index_to_word(idx, index):\n",
    "    return []\n",
    "\n",
    "def tensor_from_sentence(idx, sentence):\n",
    "    indexes = index_from_sentence(idx, sentence)\n",
    "    # Can add EOS Token Here if needed\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "def tensors_from_pair(pair):\n",
    "    input_tensor = tensor_from_sentence(roman_word_to_ix, pair[0])\n",
    "    target_tensor = tensor_from_sentence(urdu_word_to_ix, pair[1])\n",
    "    return (input_tensor, target_tensor)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert a sentence into vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 18, 19, 20, 21, 22, 1, 21, 8, 23]\n",
      "(tensor([[91],\n",
      "        [ 8],\n",
      "        [92],\n",
      "        [93],\n",
      "        [22],\n",
      "        [94],\n",
      "        [40],\n",
      "        [95],\n",
      "        [96],\n",
      "        [97]]), tensor([[90],\n",
      "        [22],\n",
      "        [91],\n",
      "        [92],\n",
      "        [21],\n",
      "        [93],\n",
      "        [41],\n",
      "        [94],\n",
      "        [95],\n",
      "        [96]]))\n"
     ]
    }
   ],
   "source": [
    "print(index_from_sentence(roman_word_to_ix, roman_urdu_lines[5]))\n",
    "\n",
    "print(tensors_from_pair([roman_urdu_lines[23], urdu_lines[23]]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we write out training function. Below code is implementing a training loop for a sequence-to-sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define teach forcing ratio and max length\n",
    "teacher_forcing_ratio = 0.5\n",
    "MAX_LENGTH = 10\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = decoder_input = input_tensor[-1].unsqueeze(0) \n",
    "    \n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "  \n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            \n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            \n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            \n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if di + 1 == target_length:\n",
    "                break\n",
    "\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensors_from_pair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_token = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensor_from_sentence(roman_word_to_ix, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = input_tensor[0].view(1, 1)\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            # decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if di == max_length - 1 or topi.item() == PAD_token:\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(REVERSE_urdu_word_to_ix[topi.item()])\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "        \n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 43s (- 6m 33s) (500 10%) 3.6637\n",
      "1m 29s (- 5m 58s) (1000 20%) 2.0055\n",
      "2m 13s (- 5m 11s) (1500 30%) 0.7091\n",
      "55m 5s (- 82m 38s) (2000 40%) 0.4111\n",
      "56m 5s (- 56m 5s) (2500 50%) 0.3374\n",
      "57m 12s (- 38m 8s) (3000 60%) 0.2350\n",
      "58m 13s (- 24m 57s) (3500 70%) 0.2511\n",
      "59m 9s (- 14m 47s) (4000 80%) 0.1860\n",
      "60m 5s (- 6m 40s) (4500 90%) 0.1494\n",
      "61m 4s (- 0m 0s) (5000 100%) 0.1635\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(len(vocab_roman), hidden_size,).to(device)\n",
    "attn_decoder1 = DecoderRNN( len(vocab_urdu), hidden_size).to(device)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, 5000, print_every=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words,_ = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> woh mein bhartih jnta party mein shaamil hotay hue reyasti\n",
      "= وہ میں بھارتیہ جنتا پارٹی میں شامل ہوتے ہوئے ریاستی\n",
      "< وہ تا میں لاہور ہائی کورٹ بار ایسوسی ایشن\n",
      "\n",
      "> woh taa mein Lahore high court baar association ke secretary\n",
      "= وہ تا میں لاہور ہائی کورٹ بار ایسوسی ایشن کے\n",
      "< وہ تا میں لاہور ہائی کورٹ بار ایسوسی ایشن\n",
      "\n",
      "> woh mein misar ki bastii mhmodih ke aik ilm dost\n",
      "= وہ میں مصر کی بستی محمودیہ کے ایک علم دوست\n",
      "< وہ تا میں لاہور ہائی کورٹ بار ایسوسی ایشن\n",
      "\n",
      "> wakeel wala railway station Pakistan mein waqay hai\n",
      "= وکیل والا ریلوے اسٹیشن پاکستان میں واقع ہے\n",
      "< وکیل قانون ایک شخص جسے دوسرے شخص کی جگہ\n",
      "\n",
      "> wagar nah dil to usay rokna hi chahta hai\n",
      "= وگر نہ دل تو اسے روکنا ہی چاہتا ہے\n",
      "< وہ میں اقوام متحدہ گئے اور کشمیر پر اپنا\n",
      "\n",
      "> woh mein wazeer taleem bhi reh chuke hain\n",
      "= وہ میں وزیر تعلیم بھی رہ چکے ہیں\n",
      "< وہ تا میں لاہور ہائی کورٹ بار ایسوسی ایشن\n",
      "\n",
      "> woh ke aam intikhabaat mein Pasheen ke halqa se rukan\n",
      "= وہ کے عام انتخابات میں پشین کے حلقہ سے رکن\n",
      "< وہ تا میں لاہور ہائی کورٹ بار ایسوسی ایشن\n",
      "\n",
      "> wakeel wala railway station Pakistan mein waqay hai\n",
      "= وکیل والا ریلوے اسٹیشن پاکستان میں واقع ہے\n",
      "< وکیل قانون ایک شخص جسے دوسرے شخص کی جگہ\n",
      "\n",
      "> woh mein wazeer taleem bhi reh chuke hain\n",
      "= وہ میں وزیر تعلیم بھی رہ چکے ہیں\n",
      "< وہ تا میں لاہور ہائی کورٹ بار ایسوسی ایشن\n",
      "\n",
      "> woh mein Oval ki tareekhi kamyabi haasil karne wali Pakistani\n",
      "= وہ میں اوول کی تاریخی کامیابی حاصل کرنے والی پاکستانی\n",
      "< وہ تا میں لاہور ہائی کورٹ بار ایسوسی ایشن\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder1, attn_decoder1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(1), self.hidden_size).to(device)\n",
    "        out, hidden = self.rnn(x, h0)\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m valid_roman_urdu \u001b[39m=\u001b[39m [to_tensor(s) \u001b[39mfor\u001b[39;00m s, _ \u001b[39min\u001b[39;00m valid_data \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(s) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m]\n\u001b[0;32m     25\u001b[0m valid_urdu \u001b[39m=\u001b[39m [to_tensor(s) \u001b[39mfor\u001b[39;00m _, s \u001b[39min\u001b[39;00m valid_data \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(s) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m]\n\u001b[1;32m---> 27\u001b[0m \u001b[39mprint\u001b[39m(train_roman_urdu\u001b[39m.\u001b[39;49msize())\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "# Split the data into training and validation sets\n",
    "split_idx = int(0.8 * len(data))\n",
    "train_data = data[:split_idx]\n",
    "valid_data = data[split_idx:]\n",
    "\n",
    "# Convert the sentences to PyTorch tensors\n",
    "def to_tensor(sentence):\n",
    "    \n",
    "    token_ids = []\n",
    "    for word in sentence:\n",
    "        if word in vocab:\n",
    "            token_ids.append(vocab[word])\n",
    "        else:\n",
    "            token_ids.append(vocab['<unk>'])\n",
    "    return torch.tensor(token_ids, dtype=torch.long)\n",
    "\n",
    "vocab = torchtext.vocab.build_vocab_from_iterator(\n",
    "    [word for sentence in roman_urdu_sentences + urdu_sentences for word in sentence],\n",
    "    specials=[\"<pad>\", \"<unk>\"]\n",
    ")\n",
    "\n",
    "train_roman_urdu = [to_tensor(s) for s, _ in train_data if len(s) > 0]\n",
    "train_urdu = [to_tensor(s) for _, s in train_data if len(s) > 0]\n",
    "valid_roman_urdu = [to_tensor(s) for s, _ in valid_data if len(s) > 0]\n",
    "valid_urdu = [to_tensor(s) for _, s in valid_data if len(s) > 0]\n",
    "\n",
    "print(train_roman_urdu.size())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mlflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[164], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmlflow\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'mlflow'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
