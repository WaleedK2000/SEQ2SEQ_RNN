{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print('hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'eel aik aisi shakhsiyat ko kaha jata hai ke jo dosray apne Sarif ki janib se ya uski baabat guftagu kere is mazmoon mein yeh guftagu qanoon se mutaliq'\n",
    "\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./training_data/Roman-Urdu.txt', 'r', encoding='utf-8') as f:\n",
    "    romanUrduLines = f.readlines()[:200]\n",
    "\n",
    "with open('./training_data/Urdu.txt', 'r', encoding='utf-8') as f:\n",
    "    urduLines = f.readlines()[:200]\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(linesSrc[0:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srcTokens = []\n",
    "trgTokens = []\n",
    "\n",
    "pair = (0,0)\n",
    "with open ('./training_data/Roman-Urdu.txt') as sourceFile, open ('./training_data/Urdu.txt') as targetFile:\n",
    "    srcTokens = []\n",
    "    trgTokens = []\n",
    "    for i in range( len(sourceFile.)):\n",
    "        srcLine = sourceFile[i]\n",
    "        srcLine.replace('/n', ' ')\n",
    "        srcTokens = srcLine.split(' ')\n",
    "\n",
    "        trgLine = targetFile[i]\n",
    "        trgLine.replace('/n', ' ')\n",
    "        trgTokens = line.split(' ')\n",
    "\n",
    "        pair = (srcTokens[3], trgToken[3])\n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "print(srcTokens[0:25], trgTokens[0:25])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = open('./training_data/Roman-Urdu.txt')\n",
    "doc_source = source.read()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Class to load data\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.source = open('./training_data/Roman-Urdu.txt', 'r', encoding='utf-8').read().split('\\n')\n",
    "        self.target = open('./training_data/Urdu.txt', 'r', encoding='utf-8').read().split('\\n')\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        source_sample = self.source[idx]\n",
    "        target_sample = self.target[idx]\n",
    "\n",
    "        return source_sample, target_sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yourDataset = Dataset()\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "        yourDataset,\n",
    "        batch_size=8,\n",
    "        num_workers=0,\n",
    "        shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataloader.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./training_data/Roman-Urdu.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    roman_urdu_data = f.read()\n",
    "\n",
    "with open(\"./training_data/Urdu.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    urdu_data = f.read()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['وکیل ', 'وکیل    ', 'وکیل     ایک ایسی شخصیت کو کہا جاتا ہے کہ جو دوسرے  اپنے صارف  کی جانب سے یا اسکی بابت گفتگو کرے   اس مضمون میں یہ گفتگو قانون سے متعلق تصور کی گئی ہے اور اس وجہ سے یہ مضمون صرف قانونی وکلا  کے بارے میں ذکر کرتا ہے', 'وکیل  قانون ', 'وکیل  قانون  ایک شخص جسے دوسرے شخص کی جگہ کام کرنے یا     کی نمائندگی کرنے کا اختیار حاصل ہوتا ہے  وکیل  سفر  ایک شخص جو تعطیلات اور سفر کا بندوبست کرتا ہے  خفیہ وکیل  ایک جاسوس ', 'وکیل بازار ضلع   لاطینی       ترکمانستان کا ایک ترکمانستان کے اضلاع جو صوبہ ماری میں واقع ہے', 'وکیل والا ریلوے اسٹیشن وکیل والا میں واقع ہے', 'وکیل والا ریلوے اسٹیشن پاکستان میں واقع ہے', 'وکیپیڈیا', 'وکیپیڈیا انگریزی پر یہ تصویر ميں نے ہی اپلوڈ کی تھی', 'وکیپیڈیا خود کسی بھی ترتیب کو نافذ نہیں کرتا', 'وکیپیڈیا لوگو کے لیے نستعلیق خطاطی', 'وکیپیڈیا میں اس سطر سے آپ تصاویر سادہ طریقے سے شامل کرسکتے ہیں ', 'وکیپیڈیا میں خوش آمدید', 'وکیپیڈیا میں کسی کے کو اس بات کی وجہ سے فوقیت حاصل نہیں کہ اس نے کس قدر وکیپیڈیا میں اضافہ کیا ہے', 'وکیپیڈیا پالیسی ', 'وکیپیڈیا پر صرف   ہزار مضمون ہیں جن میں سے اصل مضمون شاید   ہزار ہونگے', 'وکیپیڈیا پر ہر صارف کا ابنا ایک صفحہ ہوتا ہے', 'وکیپیڈیا کے اعدادوشمار', 'وکیپیڈیا کے صارفین کی اصل تصاویر', 'وکیپیڈیا کے کسی بھی صفحہ کے دائیں جانب  تلاش کا خانہ  نظر آتا ہے', 'وگر نہ دل تو اسے روکنا ہی چاہتا ہے', 'وگن شہر ضلع لاڑکانہ سندھ کا ایک خوبصورت شہر ہے  یہ انڈس ہائی وے پر لاڑکانہ سے   کلومیٹر کے فاصلہ پر واقع ہے', 'وھاں کے رہنے والوں کا تمدن بھی حضرت نوح علیہ السلام  حضرت ہود علیہ السلام اور حضرت شعیب علیہ السلام کی اقوام سے زیادہ ترقی یافتہ تھا', 'وہ       اور وہ ام  الکلامی', 'وہ     میں آئی سی سی کی جانب سے ٹی    فارمیٹ میں  سال کی بہترین کار کردگی  کا اعزاز بھی حاصل کر چکا ہے جو اسے ویسٹ انڈیز کے خلاف   گیندوں پر   دوڑیں  رنز  بنا نے پر ملا ', 'وہ    تا    الپ ارسلان کے دور حکومت اور    تا    ملک شاہ اول کے دور حکومت میں وزارت کے عہدے پر فائز رہا', 'وہ    تا    میں لاہور ہائی کورٹ بار ایسوسی ایشن کے سیکرٹری منتخب ہوئے', 'وہ    دسمبر    کو مختصر علالت کے بعد کراچی کے ایک امراض قلب کے ہسپتال میں خالق  حقیقی سے جاملے  اور وہیں نارتھ کراچی کے محمد شاہ قبرستان میں آسودہ خاک ہیں', 'وہ    سے    تک بھارت کے وزیر اعظم رہے', 'وہ    سے لے کر    تک حکومت پاکستان کے وکیل رہے', 'وہ    میں   برس کی عمر میں انتقال کرگئے', 'وہ    میں استنبول میں پیدا ہوئے', 'وہ    میں اقوام متحدہ گئے اور کشمیر پر اپنا موقف بیان کیا', 'وہ    میں امریکی فضائیہ کے شائع کردہ پوسٹر تاریخ کے   عظیم ترین ہوا باز میں واحد خاتون تھیں', 'وہ    میں انسان کی قوت  مدافعت کو کمزور کر دینے والی ایک مہلک بیماری   میں مبتلا ہو گئے تھے', 'وہ    میں اوول کی تاریخی کامیابی حاصل کرنے والی پاکستانی ٹیم کا بھی حصہ تھے', 'وہ    میں بھارتیہ جنتا پارٹی میں شامل ہوتے ہوئے ریاستی سیاست میں کود پڑے', 'وہ    میں دمشق میں پیدا ہوئے', 'وہ    میں مرکزی اردو بورڈ کے ڈائریکٹر مقرر ہوئے جو بعد میں اردو سائنس بورڈ میں تبدیل ہوگیا', 'وہ    میں مصر کی بستی محمودیہ کے ایک علم دوست اور دیندار گھرانے میں پیدا ہوئے', 'وہ    میں وزیر تعلیم بھی رہ چکے ہیں', 'وہ    میں کراچی میونسپلٹی کے رکن بنے اور    تک اس کے رکن رہے', 'وہ    میں کینیڈا میں ڈیڑھ سال گزارے کے بعد پاکستان واپس آئیں لیکن گوجرانوالہ میں ایک شو کے دوران انہیں پولیس نے حراست میں لے لیا اور ان پر عریانی اور لچر ڈانس کا الزام لگا کر حوالات میں بھیج دیا گیا', 'وہ    کو میرٹھ میں مولانا عبدالعلیم صدیقی کے گھر پیدا ہوئے', 'وہ    کے اوائل میں حمایت علی کراچی آکر ریڈیو پاکستان کا حصہ بنے', 'وہ    کے عام انتخابا   میں ایک بار پھر منتخب ہو ئے اور   اپریل    سے ڈپٹی اسپیکر بلوچستان اسمبلی کے عہدے پر فائز ہیں', 'وہ    کے عام انتخابات میں پشین کے حلقہ    سے رکن بلوچستان صوبائی اسمبلی منتخب ہو ئے ', 'وہ   اسلامی جمہوریہ پاکستان  کے سخت مخالف تھے', 'وہ   اپریل    سے  اکتوبر    تک بلوچستان کے وزیر اعلی  بھی رہے']\n"
     ]
    }
   ],
   "source": [
    "print(urdu_data.splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "roman_urdu_lines = roman_urdu_data.splitlines()\n",
    "urdu_lines = urdu_data.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wakeel', 'wakeel', 'wakeel aik aisi shakhsiyat ko kaha jata hai ke jo dosray apne Sarif ki janib se ya uski baabat guftagu kere is mazmoon mein yeh guftagu qanoon se mutaliq tasawwur ki gayi hai aur is wajah se yeh mazmoon sirf qanooni wukla ke baray mein zikar karta hai', 'wakeel qanoon', 'wakeel qanoon 1 shakhs jisay dosray shakhs ki jagah kaam karne ya ki numaindagi karne ka ikhtiyar haasil hota hai wakeel safar 1 shakhs jo tatilat aur safar ka bandobast karta hai khufia wakeel aik jasoos', 'wakeel bazaar zila lateeni turkmenistan ka aik turkmenistan ke azlaa jo soobah maari mein waqay hai', 'wakeel wala railway station wakeel wala mein waqay hai', 'wakeel wala railway station Pakistan mein waqay hai', 'wikipedia', 'wikipedia angrezi par yeh tasweer mein ne hi aplod ki thi', 'wikipedia khud kisi bhi tarteeb ko nafiz nahi karta', 'wikipedia logo ke liye nastalik khtati', 'wikipedia mein is satar se aap tasaveer saada tareeqay se shaamil kar saktay hain', 'wikipedia mein khush aamdeed', 'wikipedia mein kisi ke ko is baat ki wajah se foqiat haasil nahi ke is ne kis qader wikipedia mein izafah kya hai', 'wikipedia policy', 'wikipedia par sirf hazaar mazmoon hain jin mein se asal mazmoon shayad hazaar hunge', 'wikipedia par har Sarif ka abna aik safha hota hai', 'wikipedia ke adad o shumaar', 'wikipedia ke sarfeen ki asal tasaveer', 'wikipedia ke kisi bhi safha ke dayen janib talaash ka khanah nazar aata hai', 'wagar nah dil to usay rokna hi chahta hai', 'vgn shehar zila larkana Sindh ka aik khobsorat shehar hai yeh indus high way par larkana se kilometer ke faasla par waqay hai', 'vhan ke rehne walon ka tamaddun bhi hazrat Nooh aleh salam hazrat Hood aleh salam aur hazrat Shoiab aleh salam ki aqwam se ziyada taraqqi Yafta tha', 'woh aur woh umm alklami', 'woh mein aayi si si ki janib se tea farmit mein saal ki behtareen car kardagi ka aizaz bhi haasil kar chuka hai jo usay west indies ke khilaaf gaindon par dorhin runs bana ne par mila', 'woh taa Alap Arsalan ke daur hukoomat aur taa malik Shah awwal ke daur hukoomat mein wizarat ke ohday par Faiz raha', 'woh taa mein Lahore high court baar association ke secretary muntakhib hue', 'woh decemeber ko mukhtasir alalat ke baad Karachi ke aik amraaz qalb ke hospital mein khaaliq haqeeqi se jamilay aur wahein north Karachi ke Mohammad Shah qabrustan mein aasooda khaak hain', 'woh se taq Bharat ke Wazeer e Azam rahay', 'woh se le kar taq hukoomat Pakistan ke wakeel rahay', 'woh mein baras ki Umar mein intqaal kargaye', 'woh mein istanbul mein peda hue', 'woh mein aqwam mutahidda gay aur Kashmir par apna muaqqaf bayan kya', 'woh mein Amrici fazaiya ke shaya kardah poster tareekh ke azeem tareen sun-hwa baz mein wahid khatoon theen', 'woh mein ensaan ki qowat mudafat ko kamzor kar dainay wali aik mohlik bemari mein mubtala ho gay they', 'woh mein Oval ki tareekhi kamyabi haasil karne wali Pakistani team ka bhi hissa they', 'woh mein bhartih jnta party mein shaamil hotay hue reyasti siyasat mein kood parre', 'woh mein Dimashq mein peda hue', 'woh mein markazi urdu board ke director muqarrar hue jo baad mein urdu science board mein tabdeel hogaya', 'woh mein misar ki bastii mhmodih ke aik ilm dost aur deendar gharane mein peda hue', 'woh mein wazeer taleem bhi reh chuke hain', 'woh mein Karachi myonsplti ke rukan banay aur taq is ke rukan rahay', 'woh mein canada mein daidh saal guzaray ke baad Pakistan wapas ayen lekin Gujranwala mein aik show ke douran inhen police ne hirasat mein le liya aur un par uryani aur lachar dance ka ilzaam laga kar hawalaat mein bhaij diya gaya', 'woh ko mirth mein molana abdalalim Siddiqui ke ghar peda hue', 'woh ke awail mein himayat Ali Karachi aakar radio Pakistan ka hissa banay', 'woh ke aam antkhaba mein aik baar phir muntakhib ho ye aur April se deputy speaker Balochistan assembly ke ohday par Faiz hain', 'woh ke aam intikhabaat mein Pasheen ke halqa se rukan Balochistan sobai assembly muntakhib ho ye', 'woh islami jamhooria Pakistan ke sakht mukhalif they', 'woh April se october taq Balochistan ke wazeer Alla bhi rahay'] ['وکیل', 'قانون']\n"
     ]
    }
   ],
   "source": [
    "print(roman_urdu_lines, urdu_lines[3].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUniqueWords(lines):\n",
    "\n",
    "    vocab = []\n",
    "    for i in lines:\n",
    "        toks = i.split()\n",
    "\n",
    "        for j in toks:\n",
    "            if j not in vocab:\n",
    "                vocab.append(j)\n",
    "\n",
    "    return vocab"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Vocab of roman urdu and urdu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext.vocab as vocab\n",
    "\n",
    "vocab_roman = getUniqueWords(roman_urdu_lines)\n",
    "vocab_urdu = getUniqueWords(urdu_lines)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Urdu Vocab only first 35 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "337\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['وکیل',\n",
       " 'ایک',\n",
       " 'ایسی',\n",
       " 'شخصیت',\n",
       " 'کو',\n",
       " 'کہا',\n",
       " 'جاتا',\n",
       " 'ہے',\n",
       " 'کہ',\n",
       " 'جو',\n",
       " 'دوسرے',\n",
       " 'اپنے',\n",
       " 'صارف',\n",
       " 'کی',\n",
       " 'جانب',\n",
       " 'سے',\n",
       " 'یا',\n",
       " 'اسکی',\n",
       " 'بابت',\n",
       " 'گفتگو',\n",
       " 'کرے',\n",
       " 'اس',\n",
       " 'مضمون',\n",
       " 'میں',\n",
       " 'یہ',\n",
       " 'قانون',\n",
       " 'متعلق',\n",
       " 'تصور',\n",
       " 'گئی',\n",
       " 'اور',\n",
       " 'وجہ',\n",
       " 'صرف',\n",
       " 'قانونی',\n",
       " 'وکلا',\n",
       " 'کے']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(vocab_urdu))\n",
    "vocab_urdu[:35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "339\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['wakeel',\n",
       " 'aik',\n",
       " 'aisi',\n",
       " 'shakhsiyat',\n",
       " 'ko',\n",
       " 'kaha',\n",
       " 'jata',\n",
       " 'hai',\n",
       " 'ke',\n",
       " 'jo',\n",
       " 'dosray',\n",
       " 'apne',\n",
       " 'Sarif',\n",
       " 'ki',\n",
       " 'janib',\n",
       " 'se',\n",
       " 'ya',\n",
       " 'uski',\n",
       " 'baabat',\n",
       " 'guftagu',\n",
       " 'kere',\n",
       " 'is',\n",
       " 'mazmoon',\n",
       " 'mein',\n",
       " 'yeh',\n",
       " 'qanoon',\n",
       " 'mutaliq',\n",
       " 'tasawwur',\n",
       " 'gayi',\n",
       " 'aur',\n",
       " 'wajah',\n",
       " 'sirf',\n",
       " 'qanooni',\n",
       " 'wukla',\n",
       " 'baray']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(vocab_roman))\n",
    "vocab_roman[:35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = list(zip(vocab_roman, vocab_urdu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'wakeel': 0, 'aik': 1, 'aisi': 2, 'shakhsiyat': 3, 'ko': 4, 'kaha': 5, 'jata': 6, 'hai': 7, 'ke': 8, 'jo': 9, 'dosray': 10, 'apne': 11, 'Sarif': 12, 'ki': 13, 'janib': 14, 'se': 15, 'ya': 16, 'uski': 17, 'baabat': 18, 'guftagu': 19, 'kere': 20, 'is': 21, 'mazmoon': 22, 'mein': 23, 'yeh': 24, 'qanoon': 25, 'mutaliq': 26, 'tasawwur': 27, 'gayi': 28, 'aur': 29, 'wajah': 30, 'sirf': 31, 'qanooni': 32, 'wukla': 33, 'baray': 34, 'zikar': 35, 'karta': 36, '1': 37, 'shakhs': 38, 'jisay': 39, 'jagah': 40, 'kaam': 41, 'karne': 42, 'numaindagi': 43, 'ka': 44, 'ikhtiyar': 45, 'haasil': 46, 'hota': 47, 'safar': 48, 'tatilat': 49, 'bandobast': 50, 'khufia': 51, 'jasoos': 52, 'bazaar': 53, 'zila': 54, 'lateeni': 55, 'turkmenistan': 56, 'azlaa': 57, 'soobah': 58, 'maari': 59, 'waqay': 60, 'wala': 61, 'railway': 62, 'station': 63, 'Pakistan': 64, 'wikipedia': 65, 'angrezi': 66, 'par': 67, 'tasweer': 68, 'ne': 69, 'hi': 70, 'aplod': 71, 'thi': 72, 'khud': 73, 'kisi': 74, 'bhi': 75, 'tarteeb': 76, 'nafiz': 77, 'nahi': 78, 'logo': 79, 'liye': 80, 'nastalik': 81, 'khtati': 82, 'satar': 83, 'aap': 84, 'tasaveer': 85, 'saada': 86, 'tareeqay': 87, 'shaamil': 88, 'kar': 89, 'saktay': 90, 'hain': 91, 'khush': 92, 'aamdeed': 93, 'baat': 94, 'foqiat': 95, 'kis': 96, 'qader': 97, 'izafah': 98, 'kya': 99, 'policy': 100, 'hazaar': 101, 'jin': 102, 'asal': 103, 'shayad': 104, 'hunge': 105, 'har': 106, 'abna': 107, 'safha': 108, 'adad': 109, 'o': 110, 'shumaar': 111, 'sarfeen': 112, 'dayen': 113, 'talaash': 114, 'khanah': 115, 'nazar': 116, 'aata': 117, 'wagar': 118, 'nah': 119, 'dil': 120, 'to': 121, 'usay': 122, 'rokna': 123, 'chahta': 124, 'vgn': 125, 'shehar': 126, 'larkana': 127, 'Sindh': 128, 'khobsorat': 129, 'indus': 130, 'high': 131, 'way': 132, 'kilometer': 133, 'faasla': 134, 'vhan': 135, 'rehne': 136, 'walon': 137, 'tamaddun': 138, 'hazrat': 139, 'Nooh': 140, 'aleh': 141, 'salam': 142, 'Hood': 143, 'Shoiab': 144, 'aqwam': 145, 'ziyada': 146, 'taraqqi': 147, 'Yafta': 148, 'tha': 149, 'woh': 150, 'umm': 151, 'alklami': 152, 'aayi': 153, 'si': 154, 'tea': 155, 'farmit': 156, 'saal': 157, 'behtareen': 158, 'car': 159, 'kardagi': 160, 'aizaz': 161, 'chuka': 162, 'west': 163, 'indies': 164, 'khilaaf': 165, 'gaindon': 166, 'dorhin': 167, 'runs': 168, 'bana': 169, 'mila': 170, 'taa': 171, 'Alap': 172, 'Arsalan': 173, 'daur': 174, 'hukoomat': 175, 'malik': 176, 'Shah': 177, 'awwal': 178, 'wizarat': 179, 'ohday': 180, 'Faiz': 181, 'raha': 182, 'Lahore': 183, 'court': 184, 'baar': 185, 'association': 186, 'secretary': 187, 'muntakhib': 188, 'hue': 189, 'decemeber': 190, 'mukhtasir': 191, 'alalat': 192, 'baad': 193, 'Karachi': 194, 'amraaz': 195, 'qalb': 196, 'hospital': 197, 'khaaliq': 198, 'haqeeqi': 199, 'jamilay': 200, 'wahein': 201, 'north': 202, 'Mohammad': 203, 'qabrustan': 204, 'aasooda': 205, 'khaak': 206, 'taq': 207, 'Bharat': 208, 'Wazeer': 209, 'e': 210, 'Azam': 211, 'rahay': 212, 'le': 213, 'baras': 214, 'Umar': 215, 'intqaal': 216, 'kargaye': 217, 'istanbul': 218, 'peda': 219, 'mutahidda': 220, 'gay': 221, 'Kashmir': 222, 'apna': 223, 'muaqqaf': 224, 'bayan': 225, 'Amrici': 226, 'fazaiya': 227, 'shaya': 228, 'kardah': 229, 'poster': 230, 'tareekh': 231, 'azeem': 232, 'tareen': 233, 'sun-hwa': 234, 'baz': 235, 'wahid': 236, 'khatoon': 237, 'theen': 238, 'ensaan': 239, 'qowat': 240, 'mudafat': 241, 'kamzor': 242, 'dainay': 243, 'wali': 244, 'mohlik': 245, 'bemari': 246, 'mubtala': 247, 'ho': 248, 'they': 249, 'Oval': 250, 'tareekhi': 251, 'kamyabi': 252, 'Pakistani': 253, 'team': 254, 'hissa': 255, 'bhartih': 256, 'jnta': 257, 'party': 258, 'hotay': 259, 'reyasti': 260, 'siyasat': 261, 'kood': 262, 'parre': 263, 'Dimashq': 264, 'markazi': 265, 'urdu': 266, 'board': 267, 'director': 268, 'muqarrar': 269, 'science': 270, 'tabdeel': 271, 'hogaya': 272, 'misar': 273, 'bastii': 274, 'mhmodih': 275, 'ilm': 276, 'dost': 277, 'deendar': 278, 'gharane': 279, 'wazeer': 280, 'taleem': 281, 'reh': 282, 'chuke': 283, 'myonsplti': 284, 'rukan': 285, 'banay': 286, 'canada': 287, 'daidh': 288, 'guzaray': 289, 'wapas': 290, 'ayen': 291, 'lekin': 292, 'Gujranwala': 293, 'show': 294, 'douran': 295, 'inhen': 296, 'police': 297, 'hirasat': 298, 'liya': 299, 'un': 300, 'uryani': 301, 'lachar': 302, 'dance': 303, 'ilzaam': 304, 'laga': 305, 'hawalaat': 306, 'bhaij': 307, 'diya': 308, 'gaya': 309, 'mirth': 310, 'molana': 311, 'abdalalim': 312, 'Siddiqui': 313, 'ghar': 314, 'awail': 315, 'himayat': 316, 'Ali': 317, 'aakar': 318, 'radio': 319, 'aam': 320, 'antkhaba': 321, 'phir': 322, 'ye': 323, 'April': 324, 'deputy': 325, 'speaker': 326, 'Balochistan': 327, 'assembly': 328, 'intikhabaat': 329, 'Pasheen': 330, 'halqa': 331, 'sobai': 332, 'islami': 333, 'jamhooria': 334, 'sakht': 335, 'mukhalif': 336, 'october': 337, 'Alla': 338}\n",
      "{'وکیل': 0, 'ایک': 1, 'ایسی': 2, 'شخصیت': 3, 'کو': 4, 'کہا': 5, 'جاتا': 6, 'ہے': 7, 'کہ': 8, 'جو': 9, 'دوسرے': 10, 'اپنے': 11, 'صارف': 12, 'کی': 13, 'جانب': 14, 'سے': 15, 'یا': 16, 'اسکی': 17, 'بابت': 18, 'گفتگو': 19, 'کرے': 20, 'اس': 21, 'مضمون': 22, 'میں': 23, 'یہ': 24, 'قانون': 25, 'متعلق': 26, 'تصور': 27, 'گئی': 28, 'اور': 29, 'وجہ': 30, 'صرف': 31, 'قانونی': 32, 'وکلا': 33, 'کے': 34, 'بارے': 35, 'ذکر': 36, 'کرتا': 37, 'شخص': 38, 'جسے': 39, 'جگہ': 40, 'کام': 41, 'کرنے': 42, 'نمائندگی': 43, 'کا': 44, 'اختیار': 45, 'حاصل': 46, 'ہوتا': 47, 'سفر': 48, 'تعطیلات': 49, 'بندوبست': 50, 'خفیہ': 51, 'جاسوس': 52, 'بازار': 53, 'ضلع': 54, 'لاطینی': 55, 'ترکمانستان': 56, 'اضلاع': 57, 'صوبہ': 58, 'ماری': 59, 'واقع': 60, 'والا': 61, 'ریلوے': 62, 'اسٹیشن': 63, 'پاکستان': 64, 'وکیپیڈیا': 65, 'انگریزی': 66, 'پر': 67, 'تصویر': 68, 'ميں': 69, 'نے': 70, 'ہی': 71, 'اپلوڈ': 72, 'تھی': 73, 'خود': 74, 'کسی': 75, 'بھی': 76, 'ترتیب': 77, 'نافذ': 78, 'نہیں': 79, 'لوگو': 80, 'لیے': 81, 'نستعلیق': 82, 'خطاطی': 83, 'سطر': 84, 'آپ': 85, 'تصاویر': 86, 'سادہ': 87, 'طریقے': 88, 'شامل': 89, 'کرسکتے': 90, 'ہیں': 91, 'خوش': 92, 'آمدید': 93, 'بات': 94, 'فوقیت': 95, 'کس': 96, 'قدر': 97, 'اضافہ': 98, 'کیا': 99, 'پالیسی': 100, 'ہزار': 101, 'جن': 102, 'اصل': 103, 'شاید': 104, 'ہونگے': 105, 'ہر': 106, 'ابنا': 107, 'صفحہ': 108, 'اعدادوشمار': 109, 'صارفین': 110, 'دائیں': 111, 'تلاش': 112, 'خانہ': 113, 'نظر': 114, 'آتا': 115, 'وگر': 116, 'نہ': 117, 'دل': 118, 'تو': 119, 'اسے': 120, 'روکنا': 121, 'چاہتا': 122, 'وگن': 123, 'شہر': 124, 'لاڑکانہ': 125, 'سندھ': 126, 'خوبصورت': 127, 'انڈس': 128, 'ہائی': 129, 'وے': 130, 'کلومیٹر': 131, 'فاصلہ': 132, 'وھاں': 133, 'رہنے': 134, 'والوں': 135, 'تمدن': 136, 'حضرت': 137, 'نوح': 138, 'علیہ': 139, 'السلام': 140, 'ہود': 141, 'شعیب': 142, 'اقوام': 143, 'زیادہ': 144, 'ترقی': 145, 'یافتہ': 146, 'تھا': 147, 'وہ': 148, 'ام': 149, 'الکلامی': 150, 'آئی': 151, 'سی': 152, 'ٹی': 153, 'فارمیٹ': 154, 'سال': 155, 'بہترین': 156, 'کار': 157, 'کردگی': 158, 'اعزاز': 159, 'کر': 160, 'چکا': 161, 'ویسٹ': 162, 'انڈیز': 163, 'خلاف': 164, 'گیندوں': 165, 'دوڑیں': 166, 'رنز': 167, 'بنا': 168, 'ملا': 169, 'تا': 170, 'الپ': 171, 'ارسلان': 172, 'دور': 173, 'حکومت': 174, 'ملک': 175, 'شاہ': 176, 'اول': 177, 'وزارت': 178, 'عہدے': 179, 'فائز': 180, 'رہا': 181, 'لاہور': 182, 'کورٹ': 183, 'بار': 184, 'ایسوسی': 185, 'ایشن': 186, 'سیکرٹری': 187, 'منتخب': 188, 'ہوئے': 189, 'دسمبر': 190, 'مختصر': 191, 'علالت': 192, 'بعد': 193, 'کراچی': 194, 'امراض': 195, 'قلب': 196, 'ہسپتال': 197, 'خالق': 198, 'حقیقی': 199, 'جاملے': 200, 'وہیں': 201, 'نارتھ': 202, 'محمد': 203, 'قبرستان': 204, 'آسودہ': 205, 'خاک': 206, 'تک': 207, 'بھارت': 208, 'وزیر': 209, 'اعظم': 210, 'رہے': 211, 'لے': 212, 'برس': 213, 'عمر': 214, 'انتقال': 215, 'کرگئے': 216, 'استنبول': 217, 'پیدا': 218, 'متحدہ': 219, 'گئے': 220, 'کشمیر': 221, 'اپنا': 222, 'موقف': 223, 'بیان': 224, 'امریکی': 225, 'فضائیہ': 226, 'شائع': 227, 'کردہ': 228, 'پوسٹر': 229, 'تاریخ': 230, 'عظیم': 231, 'ترین': 232, 'ہوا': 233, 'باز': 234, 'واحد': 235, 'خاتون': 236, 'تھیں': 237, 'انسان': 238, 'قوت': 239, 'مدافعت': 240, 'کمزور': 241, 'دینے': 242, 'والی': 243, 'مہلک': 244, 'بیماری': 245, 'مبتلا': 246, 'ہو': 247, 'تھے': 248, 'اوول': 249, 'تاریخی': 250, 'کامیابی': 251, 'پاکستانی': 252, 'ٹیم': 253, 'حصہ': 254, 'بھارتیہ': 255, 'جنتا': 256, 'پارٹی': 257, 'ہوتے': 258, 'ریاستی': 259, 'سیاست': 260, 'کود': 261, 'پڑے': 262, 'دمشق': 263, 'مرکزی': 264, 'اردو': 265, 'بورڈ': 266, 'ڈائریکٹر': 267, 'مقرر': 268, 'سائنس': 269, 'تبدیل': 270, 'ہوگیا': 271, 'مصر': 272, 'بستی': 273, 'محمودیہ': 274, 'علم': 275, 'دوست': 276, 'دیندار': 277, 'گھرانے': 278, 'تعلیم': 279, 'رہ': 280, 'چکے': 281, 'میونسپلٹی': 282, 'رکن': 283, 'بنے': 284, 'کینیڈا': 285, 'ڈیڑھ': 286, 'گزارے': 287, 'واپس': 288, 'آئیں': 289, 'لیکن': 290, 'گوجرانوالہ': 291, 'شو': 292, 'دوران': 293, 'انہیں': 294, 'پولیس': 295, 'حراست': 296, 'لیا': 297, 'ان': 298, 'عریانی': 299, 'لچر': 300, 'ڈانس': 301, 'الزام': 302, 'لگا': 303, 'حوالات': 304, 'بھیج': 305, 'دیا': 306, 'گیا': 307, 'میرٹھ': 308, 'مولانا': 309, 'عبدالعلیم': 310, 'صدیقی': 311, 'گھر': 312, 'اوائل': 313, 'حمایت': 314, 'علی': 315, 'آکر': 316, 'ریڈیو': 317, 'عام': 318, 'انتخابا': 319, 'پھر': 320, 'ئے': 321, 'اپریل': 322, 'ڈپٹی': 323, 'اسپیکر': 324, 'بلوچستان': 325, 'اسمبلی': 326, 'انتخابات': 327, 'پشین': 328, 'حلقہ': 329, 'صوبائی': 330, 'اسلامی': 331, 'جمہوریہ': 332, 'سخت': 333, 'مخالف': 334, 'اکتوبر': 335, 'اعلی': 336}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mprint\u001b[39m(roman_word_to_ix)\n\u001b[0;32m      6\u001b[0m \u001b[39mprint\u001b[39m(urdu_word_to_ix)\n\u001b[1;32m----> 7\u001b[0m \u001b[39mprint\u001b[39m(roman_word_to_ix[\u001b[39m0\u001b[39;49m], urdu_word_to_ix[\u001b[39m0\u001b[39m])\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "# Create a dictionary that maps each word to an index\n",
    "roman_word_to_ix = {word: i for i, word in enumerate(vocab_roman)}\n",
    "urdu_word_to_ix = {word: i for i, word in enumerate(vocab_urdu)}\n",
    "\n",
    "print(roman_word_to_ix)\n",
    "print(urdu_word_to_ix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "        336])\n",
      "tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "        336, 337, 338])\n"
     ]
    }
   ],
   "source": [
    "# Convert To indeces\n",
    "\n",
    "urdu_word_indices = torch.tensor([urdu_word_to_ix[word] for word in vocab_urdu], dtype=torch.long)\n",
    "\n",
    "\n",
    "roman_word_indices = torch.tensor([roman_word_to_ix[word] for word in vocab_roman], dtype=torch.long)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "        336])\n"
     ]
    }
   ],
   "source": [
    "print(urdu_word_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "        336, 337, 338])\n"
     ]
    }
   ],
   "source": [
    "print(roman_word_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "embedding = nn.Embedding(num_embeddings=500, embedding_dim=10)\n",
    "\n",
    "\n",
    "roman_embedded_words = embedding(roman_word_indices)\n",
    "urdu_embedded_words = embedding(urdu_word_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4767, -0.3058,  0.2344,  ...,  0.5078, -0.9380, -1.0690],\n",
      "        [ 1.2438, -0.1743, -1.7857,  ...,  2.1216,  0.6853,  0.8915],\n",
      "        [ 0.8893,  1.5156,  1.4799,  ..., -0.0074, -0.2525,  2.0934],\n",
      "        ...,\n",
      "        [ 0.2276,  0.1307, -1.5816,  ...,  0.8715,  0.0811,  0.3351],\n",
      "        [-0.0235, -0.9776,  1.6524,  ...,  0.1228, -0.2487, -1.1017],\n",
      "        [-0.7499,  1.1364,  0.6656,  ..., -0.4363,  0.9443, -1.5120]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(urdu_embedded_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m      4\u001b[0m embedding \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mEmbedding(num_embeddings\u001b[39m=\u001b[39m\u001b[39m10000\u001b[39m, embedding_dim\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m embeddings \u001b[39m=\u001b[39m embedding(torch\u001b[39m.\u001b[39;49mLongTensor(f))\n",
      "\u001b[1;31mValueError\u001b[0m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "embedding = nn.Embedding(num_embeddings=10000, embedding_dim=128)\n",
    "embeddings = embedding(torch.LongTensor(f))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadZipFile",
     "evalue": "File is not a zip file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadZipFile\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m glove_urdu \u001b[39m=\u001b[39m vocab\u001b[39m.\u001b[39;49mGloVe(name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m6B\u001b[39;49m\u001b[39m'\u001b[39;49m, dim\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\walee\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchtext\\vocab\\vectors.py:220\u001b[0m, in \u001b[0;36mGloVe.__init__\u001b[1;34m(self, name, dim, **kwargs)\u001b[0m\n\u001b[0;32m    218\u001b[0m url \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39murl[name]\n\u001b[0;32m    219\u001b[0m name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mglove.\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39md.txt\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(name, \u001b[39mstr\u001b[39m(dim))\n\u001b[1;32m--> 220\u001b[0m \u001b[39msuper\u001b[39;49m(GloVe, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(name, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\walee\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchtext\\vocab\\vectors.py:59\u001b[0m, in \u001b[0;36mVectors.__init__\u001b[1;34m(self, name, cache, url, unk_init, max_vectors)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdim \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munk_init \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor\u001b[39m.\u001b[39mzero_ \u001b[39mif\u001b[39;00m unk_init \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m unk_init\n\u001b[1;32m---> 59\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcache(name, cache, url\u001b[39m=\u001b[39;49murl, max_vectors\u001b[39m=\u001b[39;49mmax_vectors)\n",
      "File \u001b[1;32mc:\\Users\\walee\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchtext\\vocab\\vectors.py:102\u001b[0m, in \u001b[0;36mVectors.cache\u001b[1;34m(self, name, cache, url, max_vectors)\u001b[0m\n\u001b[0;32m    100\u001b[0m ext \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39msplitext(dest)[\u001b[39m1\u001b[39m][\u001b[39m1\u001b[39m:]\n\u001b[0;32m    101\u001b[0m \u001b[39mif\u001b[39;00m ext \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mzip\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 102\u001b[0m     \u001b[39mwith\u001b[39;00m zipfile\u001b[39m.\u001b[39;49mZipFile(dest, \u001b[39m\"\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m zf:\n\u001b[0;32m    103\u001b[0m         zf\u001b[39m.\u001b[39mextractall(cache)\n\u001b[0;32m    104\u001b[0m \u001b[39melif\u001b[39;00m ext \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mgz\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\walee\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\zipfile.py:1299\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[1;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[0m\n\u001b[0;32m   1297\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1298\u001b[0m     \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m-> 1299\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_RealGetContents()\n\u001b[0;32m   1300\u001b[0m     \u001b[39melif\u001b[39;00m mode \u001b[39min\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mx\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m   1301\u001b[0m         \u001b[39m# set the modified flag so central directory gets written\u001b[39;00m\n\u001b[0;32m   1302\u001b[0m         \u001b[39m# even if no files are added to the archive\u001b[39;00m\n\u001b[0;32m   1303\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_didModify \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\walee\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\zipfile.py:1366\u001b[0m, in \u001b[0;36mZipFile._RealGetContents\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[39mraise\u001b[39;00m BadZipFile(\u001b[39m\"\u001b[39m\u001b[39mFile is not a zip file\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1365\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m endrec:\n\u001b[1;32m-> 1366\u001b[0m     \u001b[39mraise\u001b[39;00m BadZipFile(\u001b[39m\"\u001b[39m\u001b[39mFile is not a zip file\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1367\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdebug \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1368\u001b[0m     \u001b[39mprint\u001b[39m(endrec)\n",
      "\u001b[1;31mBadZipFile\u001b[0m: File is not a zip file"
     ]
    }
   ],
   "source": [
    "glove_urdu = vocab.GloVe(name='6B', dim=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import torch\n",
    "import torchtext\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Read the contents of both files and store them in separate variables\n",
    "with open(\"./training_data/Roman-Urdu.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    roman_urdu_data = f.read()\n",
    "\n",
    "with open(\"./training_data/Urdu.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    urdu_data = f.read()\n",
    "\n",
    "# Split the data in both files into sentences\n",
    "roman_urdu_sentences = re.split(r\"[.\\n]\", roman_urdu_data)\n",
    "urdu_sentences = re.split(r\"[.\\n]\", urdu_data)\n",
    "\n",
    "# Remove any unnecessary characters and tokenize the sentences\n",
    "roman_urdu_sentences = [re.sub(r\"[^a-zA-Z0-9آ-ی]+\", \" \", s).strip().split()[:10] for s in roman_urdu_sentences]\n",
    "urdu_sentences = [re.sub(r\"[^آ-ی۔]+\", \" \", s).strip().split()[:10] for s in urdu_sentences]\n",
    "\n",
    "# Create separate lists for Urdu and Roman Urdu sentences\n",
    "data = list(zip(roman_urdu_sentences, urdu_sentences))\n",
    "random.shuffle(data)\n",
    "roman_urdu_sentences, urdu_sentences = zip(*data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "woh وہ\n",
      "gg\n"
     ]
    }
   ],
   "source": [
    "print(roman_urdu_sentences[0][0], urdu_sentences[0][0])\n",
    "print('gg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['woh', 'ke', 'awail', 'mein', 'himayat', 'Ali', 'Karachi', 'aakar', 'radio', 'Pakistan'], ['وہ', 'ک', 'اوائل', 'میں', 'حمایت', 'علی', 'کراچی', 'آکر', 'ریڈیو', 'پاکستان']), (['woh', 'mein', 'ensaan', 'ki', 'qowat', 'mudafat', 'ko', 'kamzor', 'kar', 'dainay'], ['وہ', 'میں', 'انسان', 'کی', 'قوت', 'مدافعت', 'کو', 'کمزور', 'کر', 'دین']), (['woh', 'se', 'le', 'kar', 'taq', 'hukoomat', 'Pakistan', 'ke', 'wakeel', 'rahay'], ['وہ', 'س', 'ل', 'کر', 'تک', 'حکومت', 'پاکستان', 'ک', 'وکیل', 'رہ']), (['wikipedia', 'par', 'sirf', 'hazaar', 'mazmoon', 'hain', 'jin', 'mein', 'se', 'asal'], ['وکیپیڈیا', 'پر', 'صرف', 'ہزار', 'مضمون', 'ہیں', 'جن', 'میں', 'س', 'اصل']), (['woh', 'se', 'taq', 'Bharat', 'ke', 'Wazeer', 'e', 'Azam', 'rahay'], ['وہ', 'س', 'تک', 'بھارت', 'ک', 'وزیر', 'اعظم', 'رہ']), (['wakeel', 'wala', 'railway', 'station', 'Pakistan', 'mein', 'waqay', 'hai'], ['وکیل', 'والا', 'ریلو', 'اسٹیشن', 'پاکستان', 'میں', 'واقع', 'ہ']), (['woh', 'mein', 'markazi', 'urdu', 'board', 'ke', 'director', 'muqarrar', 'hue', 'jo'], ['وہ', 'میں', 'مرکزی', 'اردو', 'بورڈ', 'ک', 'ڈائریکٹر', 'مقرر', 'ہوئ', 'جو']), (['wikipedia', 'ke', 'sarfeen', 'ki', 'asal', 'tasaveer'], ['وکیپیڈیا', 'ک', 'صارفین', 'کی', 'اصل', 'تصاویر']), (['woh', 'ke', 'aam', 'intikhabaat', 'mein', 'Pasheen', 'ke', 'halqa', 'se', 'rukan'], ['وہ', 'ک', 'عام', 'انتخابات', 'میں', 'پشین', 'ک', 'حلقہ', 'س', 'رکن']), (['wakeel'], ['وکیل']), (['woh', 'mein', 'Oval', 'ki', 'tareekhi', 'kamyabi', 'haasil', 'karne', 'wali', 'Pakistani'], ['وہ', 'میں', 'اوول', 'کی', 'تاریخی', 'کامیابی', 'حاصل', 'کرن', 'والی', 'پاکستانی']), (['vhan', 'ke', 'rehne', 'walon', 'ka', 'tamaddun', 'bhi', 'hazrat', 'Nooh', 'aleh'], ['وھاں', 'ک', 'رہن', 'والوں', 'کا', 'تمدن', 'بھی', 'حضرت', 'نوح', 'علیہ']), (['woh', 'mein', 'bhartih', 'jnta', 'party', 'mein', 'shaamil', 'hotay', 'hue', 'reyasti'], ['وہ', 'میں', 'بھارتیہ', 'جنتا', 'پارٹی', 'میں', 'شامل', 'ہوت', 'ہوئ', 'ریاستی']), (['wakeel', 'bazaar', 'zila', 'lateeni', 'turkmenistan', 'ka', 'aik', 'turkmenistan', 'ke', 'azlaa'], ['وکیل', 'بازار', 'ضلع', 'لاطینی', 'ترکمانستان', 'کا', 'ایک', 'ترکمانستان', 'ک', 'اضلاع']), (['wikipedia', 'ke', 'kisi', 'bhi', 'safha', 'ke', 'dayen', 'janib', 'talaash', 'ka'], ['وکیپیڈیا', 'ک', 'کسی', 'بھی', 'صفحہ', 'ک', 'دائیں', 'جانب', 'تلاش', 'کا']), (['woh', 'mein', 'Amrici', 'fazaiya', 'ke', 'shaya', 'kardah', 'poster', 'tareekh', 'ke'], ['وہ', 'میں', 'امریکی', 'فضائیہ', 'ک', 'شائع', 'کردہ', 'پوسٹر', 'تاریخ', 'ک']), (['woh', 'aur', 'woh', 'umm', 'alklami'], ['وہ', 'اور', 'وہ', 'ام', 'الکلامی']), (['wikipedia', 'khud', 'kisi', 'bhi', 'tarteeb', 'ko', 'nafiz', 'nahi', 'karta'], ['وکیپیڈیا', 'خود', 'کسی', 'بھی', 'ترتیب', 'کو', 'نافذ', 'نہیں', 'کرتا']), (['wikipedia', 'logo', 'ke', 'liye', 'nastalik', 'khtati'], ['وکیپیڈیا', 'لوگو', 'ک', 'لی', 'نستعلیق', 'خطاطی']), (['woh', 'taa', 'Alap', 'Arsalan', 'ke', 'daur', 'hukoomat', 'aur', 'taa', 'malik'], ['وہ', 'تا', 'الپ', 'ارسلان', 'ک', 'دور', 'حکومت', 'اور', 'تا', 'ملک']), (['woh', 'mein', 'wazeer', 'taleem', 'bhi', 'reh', 'chuke', 'hain'], ['وہ', 'میں', 'وزیر', 'تعلیم', 'بھی', 'رہ', 'چک', 'ہیں']), (['wikipedia', 'angrezi', 'par', 'yeh', 'tasweer', 'mein', 'ne', 'hi', 'aplod', 'ki'], ['وکیپیڈیا', 'انگریزی', 'پر', 'یہ', 'تصویر', 'ميں', 'ن', 'ہی', 'اپلوڈ', 'کی']), (['wakeel', 'qanoon', '1', 'shakhs', 'jisay', 'dosray', 'shakhs', 'ki', 'jagah', 'kaam'], ['وکیل', 'قانون', 'ایک', 'شخص', 'جس', 'دوسر', 'شخص', 'کی', 'جگہ', 'کام']), (['woh', 'mein', 'baras', 'ki', 'Umar', 'mein', 'intqaal', 'kargaye'], ['وہ', 'میں', 'برس', 'کی', 'عمر', 'میں', 'انتقال', 'کرگئ']), (['wikipedia', 'mein', 'kisi', 'ke', 'ko', 'is', 'baat', 'ki', 'wajah', 'se'], ['وکیپیڈیا', 'میں', 'کسی', 'ک', 'کو', 'اس', 'بات', 'کی', 'وجہ', 'س']), (['wikipedia', 'ke', 'adad', 'o', 'shumaar'], ['وکیپیڈیا', 'ک', 'اعدادوشمار']), (['woh', 'decemeber', 'ko', 'mukhtasir', 'alalat', 'ke', 'baad', 'Karachi', 'ke', 'aik'], ['وہ', 'دسمبر', 'کو', 'مختصر', 'علالت', 'ک', 'بعد', 'کراچی', 'ک', 'ایک']), (['woh', 'mein', 'misar', 'ki', 'bastii', 'mhmodih', 'ke', 'aik', 'ilm', 'dost'], ['وہ', 'میں', 'مصر', 'کی', 'بستی', 'محمودیہ', 'ک', 'ایک', 'علم', 'دوست']), (['wikipedia', 'policy'], ['وکیپیڈیا', 'پالیسی']), (['wakeel', 'wala', 'railway', 'station', 'wakeel', 'wala', 'mein', 'waqay', 'hai'], ['وکیل', 'والا', 'ریلو', 'اسٹیشن', 'وکیل', 'والا', 'میں', 'واقع', 'ہ']), (['woh', 'mein', 'istanbul', 'mein', 'peda', 'hue'], ['وہ', 'میں', 'استنبول', 'میں', 'پیدا', 'ہوئ']), (['woh', 'mein', 'aqwam', 'mutahidda', 'gay', 'aur', 'Kashmir', 'par', 'apna', 'muaqqaf'], ['وہ', 'میں', 'اقوام', 'متحدہ', 'گئ', 'اور', 'کشمیر', 'پر', 'اپنا', 'موقف']), (['wikipedia', 'mein', 'khush', 'aamdeed'], ['وکیپیڈیا', 'میں', 'خوش', 'آمدید']), (['woh', 'mein', 'aayi', 'si', 'si', 'ki', 'janib', 'se', 'tea', 'farmit'], ['وہ', 'میں', 'آئی', 'سی', 'سی', 'کی', 'جانب', 'س', 'ٹی', 'فارمیٹ']), (['woh', 'mein', 'Karachi', 'myonsplti', 'ke', 'rukan', 'banay', 'aur', 'taq', 'is'], ['وہ', 'میں', 'کراچی', 'میونسپلٹی', 'ک', 'رکن', 'بن', 'اور', 'تک', 'اس']), (['woh', 'ke', 'aam', 'antkhaba', 'mein', 'aik', 'baar', 'phir', 'muntakhib', 'ho'], ['وہ', 'ک', 'عام', 'انتخابا', 'میں', 'ایک', 'بار', 'پھر', 'منتخب', 'ہو']), (['wakeel', 'aik', 'aisi', 'shakhsiyat', 'ko', 'kaha', 'jata', 'hai', 'ke', 'jo'], ['وکیل', 'ایک', 'ایسی', 'شخصیت', 'کو', 'کہا', 'جاتا', 'ہ', 'کہ', 'جو']), (['wikipedia', 'mein', 'is', 'satar', 'se', 'aap', 'tasaveer', 'saada', 'tareeqay', 'se'], ['وکیپیڈیا', 'میں', 'اس', 'سطر', 'س', 'آپ', 'تصاویر', 'سادہ', 'طریق', 'س']), (['woh', 'ko', 'mirth', 'mein', 'molana', 'abdalalim', 'Siddiqui', 'ke', 'ghar', 'peda'], ['وہ', 'کو', 'میرٹھ', 'میں', 'مولانا', 'عبدالعلیم', 'صدیقی', 'ک', 'گھر', 'پیدا']), (['woh', 'mein', 'Dimashq', 'mein', 'peda', 'hue'], ['وہ', 'میں', 'دمشق', 'میں', 'پیدا', 'ہوئ']), (['wakeel'], ['وکیل']), (['wikipedia', 'par', 'har', 'Sarif', 'ka', 'abna', 'aik', 'safha', 'hota', 'hai'], ['وکیپیڈیا', 'پر', 'ہر', 'صارف', 'کا', 'ابنا', 'ایک', 'صفحہ', 'ہوتا', 'ہ']), (['woh', 'taa', 'mein', 'Lahore', 'high', 'court', 'baar', 'association', 'ke', 'secretary'], ['وہ', 'تا', 'میں', 'لاہور', 'ہائی', 'کورٹ', 'بار', 'ایسوسی', 'ایشن', 'ک']), (['woh', 'islami', 'jamhooria', 'Pakistan', 'ke', 'sakht', 'mukhalif', 'they'], ['وہ', 'اسلامی', 'جمہوریہ', 'پاکستان', 'ک', 'سخت', 'مخالف', 'تھ']), (['woh', 'mein', 'canada', 'mein', 'daidh', 'saal', 'guzaray', 'ke', 'baad', 'Pakistan'], ['وہ', 'میں', 'کینیڈا', 'میں', 'ڈیڑھ', 'سال', 'گزار', 'ک', 'بعد', 'پاکستان']), (['wakeel', 'qanoon'], ['وکیل', 'قانون']), (['vgn', 'shehar', 'zila', 'larkana', 'Sindh', 'ka', 'aik', 'khobsorat', 'shehar', 'hai'], ['وگن', 'شہر', 'ضلع', 'لاڑکانہ', 'سندھ', 'کا', 'ایک', 'خوبصورت', 'شہر', 'ہ']), (['wikipedia'], ['وکیپیڈیا']), (['woh', 'April', 'se', 'october', 'taq', 'Balochistan', 'ke', 'wazeer', 'Alla', 'bhi'], ['وہ', 'اپریل', 'س', 'اکتوبر', 'تک', 'بلوچستان', 'ک', 'وزیر', 'اعلی', 'بھی']), (['wagar', 'nah', 'dil', 'to', 'usay', 'rokna', 'hi', 'chahta', 'hai'], ['وگر', 'نہ', 'دل', 'تو', 'اس', 'روکنا', 'ہی', 'چاہتا', 'ہ'])]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mprint\u001b[39m(data)\n\u001b[1;32m----> 2\u001b[0m roman_urdu_sentences\u001b[39m.\u001b[39;49msize()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "print(data)\n",
    "roman_urdu_sentences.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m valid_roman_urdu \u001b[39m=\u001b[39m [to_tensor(s) \u001b[39mfor\u001b[39;00m s, _ \u001b[39min\u001b[39;00m valid_data \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(s) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m]\n\u001b[0;32m     25\u001b[0m valid_urdu \u001b[39m=\u001b[39m [to_tensor(s) \u001b[39mfor\u001b[39;00m _, s \u001b[39min\u001b[39;00m valid_data \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(s) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m]\n\u001b[1;32m---> 27\u001b[0m \u001b[39mprint\u001b[39m(train_roman_urdu\u001b[39m.\u001b[39;49msize())\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "# Split the data into training and validation sets\n",
    "split_idx = int(0.8 * len(data))\n",
    "train_data = data[:split_idx]\n",
    "valid_data = data[split_idx:]\n",
    "\n",
    "# Convert the sentences to PyTorch tensors\n",
    "def to_tensor(sentence):\n",
    "    \n",
    "    token_ids = []\n",
    "    for word in sentence:\n",
    "        if word in vocab:\n",
    "            token_ids.append(vocab[word])\n",
    "        else:\n",
    "            token_ids.append(vocab['<unk>'])\n",
    "    return torch.tensor(token_ids, dtype=torch.long)\n",
    "\n",
    "vocab = torchtext.vocab.build_vocab_from_iterator(\n",
    "    [word for sentence in roman_urdu_sentences + urdu_sentences for word in sentence],\n",
    "    specials=[\"<pad>\", \"<unk>\"]\n",
    ")\n",
    "\n",
    "train_roman_urdu = [to_tensor(s) for s, _ in train_data if len(s) > 0]\n",
    "train_urdu = [to_tensor(s) for _, s in train_data if len(s) > 0]\n",
    "valid_roman_urdu = [to_tensor(s) for s, _ in valid_data if len(s) > 0]\n",
    "valid_urdu = [to_tensor(s) for _, s in valid_data if len(s) > 0]\n",
    "\n",
    "print(train_roman_urdu.size())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(*train_roman_urdu, *train_urdu)\n",
    "valid_dataset = TensorDataset(valid_roman_urdu, valid_urdu)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
