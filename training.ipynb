{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print('hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'eel aik aisi shakhsiyat ko kaha jata hai ke jo dosray apne Sarif ki janib se ya uski baabat guftagu kere is mazmoon mein yeh guftagu qanoon se mutaliq'\n",
    "\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./training_data/Roman-Urdu.txt', 'r', encoding='utf-8') as f:\n",
    "    romanUrduLines = f.readlines()[:200]\n",
    "\n",
    "with open('./training_data/Urdu.txt', 'r', encoding='utf-8') as f:\n",
    "    urduLines = f.readlines()[:200]\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(linesSrc[0:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srcTokens = []\n",
    "trgTokens = []\n",
    "\n",
    "pair = (0,0)\n",
    "with open ('./training_data/Roman-Urdu.txt') as sourceFile, open ('./training_data/Urdu.txt') as targetFile:\n",
    "    srcTokens = []\n",
    "    trgTokens = []\n",
    "    for i in range( len(sourceFile.)):\n",
    "        srcLine = sourceFile[i]\n",
    "        srcLine.replace('/n', ' ')\n",
    "        srcTokens = srcLine.split(' ')\n",
    "\n",
    "        trgLine = targetFile[i]\n",
    "        trgLine.replace('/n', ' ')\n",
    "        trgTokens = line.split(' ')\n",
    "\n",
    "        pair = (srcTokens[3], trgToken[3])\n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "print(srcTokens[0:25], trgTokens[0:25])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = open('./training_data/Roman-Urdu.txt')\n",
    "doc_source = source.read()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Class to load data\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.source = open('./training_data/Roman-Urdu.txt', 'r', encoding='utf-8').read().split('\\n')\n",
    "        self.target = open('./training_data/Urdu.txt', 'r', encoding='utf-8').read().split('\\n')\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        source_sample = self.source[idx]\n",
    "        target_sample = self.target[idx]\n",
    "\n",
    "        return source_sample, target_sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yourDataset = Dataset()\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "        yourDataset,\n",
    "        batch_size=8,\n",
    "        num_workers=0,\n",
    "        shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataloader.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import torch\n",
    "import torchtext\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Read the contents of both files and store them in separate variables\n",
    "with open(\"./training_data/Roman-Urdu.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    roman_urdu_data = f.read(25)\n",
    "\n",
    "with open(\"./training_data/Urdu.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    urdu_data = f.read(25)\n",
    "\n",
    "# Split the data in both files into sentences\n",
    "roman_urdu_sentences = re.split(r\"[.\\n]\", roman_urdu_data)\n",
    "urdu_sentences = re.split(r\"[.\\n]\", urdu_data)\n",
    "\n",
    "# Remove any unnecessary characters and tokenize the sentences\n",
    "roman_urdu_sentences = [re.sub(r\"[^a-zA-Z0-9آ-ی]+\", \" \", s).strip().split()[:10] for s in roman_urdu_sentences]\n",
    "urdu_sentences = [re.sub(r\"[^آ-ی۔]+\", \" \", s).strip().split()[:10] for s in urdu_sentences]\n",
    "\n",
    "# Create separate lists for Urdu and Roman Urdu sentences\n",
    "data = list(zip(roman_urdu_sentences, urdu_sentences))\n",
    "random.shuffle(data)\n",
    "roman_urdu_sentences, urdu_sentences = zip(*data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(roman_urdu_sentences[0], 'hje')\n",
    "print('gg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "split_idx = int(0.8 * len(data))\n",
    "train_data = data[:split_idx]\n",
    "valid_data = data[split_idx:]\n",
    "\n",
    "# Convert the sentences to PyTorch tensors\n",
    "def to_tensor(sentence):\n",
    "    return torch.tensor([vocab.get_stoi()[word] for word in sentence], dtype=torch.long)\n",
    "\n",
    "vocab = torchtext.vocab.build_vocab_from_iterator(\n",
    "    [word for sentence in roman_urdu_sentences + urdu_sentences for word in sentence],\n",
    "    specials=[\"<pad>\", \"<unk>\"]\n",
    ")\n",
    "\n",
    "train_roman_urdu = [to_tensor(s) for s, _ in train_data]\n",
    "train_urdu = [to_tensor(s) for _, s in train_data]\n",
    "valid_roman_urdu = [to_tensor(s) for s, _ in valid_data]\n",
    "valid_urdu = [to_tensor(s) for _, s in valid_data]\n",
    "\n",
    "train_dataset = TensorDataset(train_roman_urdu, train_urdu)\n",
    "valid_dataset = TensorDataset(valid_roman_urdu, valid_urdu)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
